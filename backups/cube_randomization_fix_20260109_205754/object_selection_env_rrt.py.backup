"""
RL Environment for Object Selection with RRT Path Planning (Option 3)
Uses actual RRT path planning in Isaac Sim for training.
Requires Isaac Sim to be running during training.
"""

import numpy as np
import time
import random
from typing import Dict, Optional, Tuple
from .object_selection_env import ObjectSelectionEnv
from .path_estimators_isaacsim import IsaacSimRRTPathEstimator
from .distance_utils import calculate_placement_position


class ObjectSelectionEnvRRT(ObjectSelectionEnv):
    """
    Environment variant that uses actual RRT path planning for reward calculation.
    
    This requires Isaac Sim to be running and a Franka controller with RRT planner.
    Training will be slower but rewards will be based on actual RRT performance.
    """
    
    def __init__(
        self,
        franka_controller=None,  # Optional for visualization
        max_objects: int = 10,
        max_steps: int = 50,
        num_cubes: int = 4,
        render_mode: Optional[str] = None,
        dynamic_obstacles: bool = True,  # Usually True for RRT training
        training_grid_size: int = 6,
        execute_picks: bool = False,  # Whether to actually execute pick-and-place
        rrt_planner=None,  # Isaac Sim RRT planner instance
        kinematics_solver=None,  # Lula kinematics solver
        articulation_kinematics_solver=None,  # Articulation kinematics solver
        franka_articulation=None  # Franka robot articulation
    ):
        super().__init__(
            franka_controller=franka_controller,
            max_objects=max_objects,
            max_steps=max_steps,
            num_cubes=num_cubes,
            render_mode=render_mode,
            dynamic_obstacles=dynamic_obstacles,
            training_grid_size=training_grid_size
        )

        self.execute_picks = execute_picks

        # Initialize Isaac Sim RRT path estimator (for training)
        # UPDATED: Cell size accounts for gripper width (15.2cm) + safety margin
        self.rrt_estimator = IsaacSimRRTPathEstimator(
            grid_size=training_grid_size,
            cell_size=0.20 if training_grid_size > 3 else 0.22,  # 20cm for 4x4+, 22cm for 3x3
            rrt_planner=rrt_planner,
            kinematics_solver=kinematics_solver,
            articulation_kinematics_solver=articulation_kinematics_solver,
            franka_articulation=franka_articulation
        )

        # RRT performance tracking
        self.rrt_planning_times = []
        self.rrt_path_lengths = []
        self.rrt_optimal_path_lengths = []  # NEW: Track optimal (straight-line) path lengths
        self.rrt_success_count = 0
        self.rrt_failure_count = 0
        self.episode_count = 0
        self.stats_print_interval = 100  # Print stats every 100 episodes

        # Per-episode tracking for detailed metrics
        self.episode_rrt_failures = 0
        self.episode_pick_failures = 0
        self.episode_collisions = 0
        self.episode_successful_picks = 0
        self.episode_unreachable_cubes = 0  # Cubes that were never reachable (terminated before attempting)

        # Track skipped cubes separately from successfully picked cubes
        self.cubes_skipped = []  # List of cube indices that were skipped (unreachable)

    def set_rrt_components(self, rrt_planner, kinematics_solver,
                          articulation_kinematics_solver, franka_articulation):
        """Set all Isaac Sim RRT components"""
        self.rrt_estimator.set_rrt_planner(rrt_planner)
        self.rrt_estimator.set_kinematics_solver(kinematics_solver)
        self.rrt_estimator.set_articulation_kinematics_solver(articulation_kinematics_solver)
        self.rrt_estimator.set_franka_articulation(franka_articulation)

    def _update_rrt_grid(self, target_cube_idx: Optional[int] = None):
        """
        Update Isaac Sim RRT occupancy grid based on current object positions and obstacles.

        Args:
            target_cube_idx: Index of target cube to exclude from obstacles (for path planning to that cube)
                           If None, all cubes are treated as obstacles (for general grid update)
        """
        # Get obstacle positions from multiple sources
        obstacle_positions = []

        # 1. Lidar detected obstacles (if available - for RRT integration)
        if self.franka_controller and hasattr(self.franka_controller, 'lidar_detected_obstacles'):
            # Extract obstacle positions from Lidar detected obstacles
            for obs_name, obs_data in self.franka_controller.lidar_detected_obstacles.items():
                obs_pos = obs_data.get('position', None)
                if obs_pos is not None:
                    obstacle_positions.append(obs_pos)

        # 2. Random obstacles (for standalone training without Isaac Sim)
        if hasattr(self, 'random_obstacle_positions') and self.random_obstacle_positions:
            obstacle_positions.extend(self.random_obstacle_positions)

        # 3. Unpicked cubes as obstacles (EXCLUDING target cube and already picked cubes)
        unpicked_cube_positions = []
        for i in range(self.total_objects):
            if i not in self.objects_picked and i != target_cube_idx:
                unpicked_cube_positions.append(self.object_positions[i])

        # Update grid with unpicked cubes (excluding target) and static obstacles
        self.rrt_estimator.update_occupancy_grid(
            object_positions=unpicked_cube_positions,
            obstacle_positions=obstacle_positions
        )

    def _is_reachable(self, obj_idx: int) -> bool:
        """
        FULL Isaac Sim RRT reachability check for action masking (overrides base class).

        Uses actual Isaac Sim RRT path planning (franka_controller.rrt.compute_path)
        to ensure agent only sees truly reachable cubes.
        This prevents the agent from learning to select blocked cubes.

        IMPORTANT: This is called during action masking for EVERY unpicked cube (~9 calls per step).
        Reachability is re-evaluated every step, so previously unreachable cubes can become reachable.

        Args:
            obj_idx: Index of object to check

        Returns:
            True if Isaac Sim RRT can find a path to the object, False otherwise
        """
        if obj_idx in self.objects_picked:
            return False  # Already picked

        # Use SAME RRT as actual pick: franka_controller.rrt.compute_path()
        # This ensures action mask and actual pick use identical reachability logic
        # Re-evaluate reachability every step (no caching)
        obj_position = self.object_positions[obj_idx]
        rrt_result = self._plan_rrt_path_to_object(obj_position, self.object_names[obj_idx], max_retries=1)

        return rrt_result["success"]

    def _calculate_reachability(self, obj_idx: int, dist_to_ee: float) -> float:
        """
        Calculate reachability for OBSERVATION (called for ALL objects).

        CRITICAL: This is called during _get_observation() for ALL objects (not just unpicked).
        Using full Isaac Sim RRT here would mean ~9 cubes × RRT planning = significant slowdown.

        Use FAST distance-based check instead. Full RRT is only used in reward calculation.

        Args:
            obj_idx: Index of object to check
            dist_to_ee: Euclidean distance from EE to object

        Returns:
            1.0 if within reachable distance, 0.0 if too far/close
        """
        # FAST: Distance-based check (no RRT planning)
        # This matches the fast check in _is_reachable()
        return 1.0 if (0.3 <= dist_to_ee <= 0.9) else 0.0

    def _calculate_reward(self, action: int) -> float:
        """
        Calculate reward using RRT path estimation.

        UPDATED: Now uses all 6 observation parameters for reward calculation:
        1. RRT path length (10 points max) - prioritizes nearest objects
        2. Distance to container (3 points max)
        3. Obstacle proximity (7 points max) - includes unpicked cubes
        4. Reachability flag (-10 penalty if unreachable)
        5. Path clearance (4 points max)
        6. Picked flag (handled via invalid action penalty)

        Additional penalties:
        - Path planning failure: -10 if RRT fails (returns 2.0×Euclidean)
        - Time penalty: -2 per step

        Bonuses:
        - First pick bonus: +5 if optimal first pick (shortest RRT path)
        - Completion bonus: +20 + time bonus (handled in step())
        """
        reward = 0.0
        obj_position = self.object_positions[action]

        # Base reward for successful pick
        reward += 10.0

        # 1. RRT path length reward (max 10 points) - INCREASED for nearest-first priority
        rrt_path_length = self.rrt_estimator.estimate_path_length(self.ee_position, obj_position)
        euclidean_distance = np.linalg.norm(obj_position[:2] - self.ee_position[:2])

        # Check if RRT planning failed (returns 2.0 × Euclidean as penalty)
        if rrt_path_length >= 2.0 * euclidean_distance:
            reward -= 10.0  # INCREASED path planning failure penalty

        # Normalize by typical path length (0.3m to 0.9m, same as A*)
        normalized_path_length = (rrt_path_length - 0.3) / 0.6
        normalized_path_length = np.clip(normalized_path_length, 0.0, 1.0)
        path_reward = 10.0 * (1.0 - normalized_path_length)  # INCREASED from 5.0
        reward += path_reward

        # 2. Distance to container reward (max 3 points)
        dist_to_container = np.linalg.norm(obj_position - self.container_position)
        container_reward = 3.0 * np.exp(-dist_to_container)
        reward += container_reward

        # 3. Obstacle proximity reward (max 7 points) - INCREASED
        # Now includes unpicked cubes as obstacles
        obstacle_score = self._calculate_obstacle_score_with_unpicked_cubes(obj_position, action)
        obstacle_reward = 7.0 * (1.0 - obstacle_score)  # INCREASED from 3.0
        reward += obstacle_reward

        # 4. Reachability penalty (-10 if unreachable) - INCREASED
        distance = np.linalg.norm(obj_position - self.ee_position)
        if not (0.3 <= distance <= 0.9):
            reward -= 10.0  # INCREASED from -5.0

        # 5. Path clearance reward (max 4 points) - INCREASED
        clearance_score = self._calculate_path_clearance(self.ee_position, obj_position)
        clearance_reward = 4.0 * clearance_score  # INCREASED from 2.0
        reward += clearance_reward

        # 6. Additional penalties for risky picks (close to masking thresholds)
        # These guide agent to prefer safer picks among valid actions
        if clearance_score < 0.30:  # Close to Layer 2 threshold (0.25)
            reward -= 5.0  # Risky pick - narrow path

        if obstacle_score > 0.60:  # Close to Layer 3 threshold (0.65)
            reward -= 5.0  # Risky pick - crowded area

        # Time penalty (encourage speed) - INCREASED
        reward -= 2.0  # INCREASED from -1.0

        # Sequential picking bonus: reward for picking closest object first
        # OPTIMIZED: Use Euclidean distance instead of RRT for first pick bonus
        # (calculating RRT for all 9 cubes would add significant overhead to first step!)
        if len(self.objects_picked) == 0:
            # First pick: bonus if it's the closest object (by Euclidean distance)
            distances = [np.linalg.norm(pos[:2] - self.ee_position[:2]) for pos in self.object_positions]
            if action == np.argmin(distances):
                reward += 5.0

        return reward

    def _plan_rrt_path_to_object(self, obj_position: np.ndarray, obj_name: str, max_retries: int = 3) -> Dict:
        """
        Plan RRT path to object position with retry logic.

        Args:
            obj_position: Target object position (x, y, z)
            obj_name: Name of the object
            max_retries: Maximum number of RRT attempts (default: 3)

        Returns:
            Dictionary with:
                - success: bool
                - path_length: float (if success)
                - path: trajectory (if success)
                - reason: str (if failure)
                - attempts: int (number of attempts made)
        """
        if self.franka_controller is None:
            return {"success": False, "reason": "no_controller", "attempts": 0}

        try:
            # Get current robot state
            current_joint_positions = self.franka_controller.franka.get_joint_positions()

            # Calculate target position (above object for picking)
            pick_height_offset = 0.15  # 15cm above object
            target_position = obj_position.copy()
            target_position[2] += pick_height_offset

            # Use Franka controller's RRT planner
            if hasattr(self.franka_controller, 'rrt') and self.franka_controller.rrt is not None:
                # Set target for RRT
                target_orientation = np.array([1.0, 0.0, 0.0, 0.0])  # Default orientation
                self.franka_controller.rrt.set_end_effector_target(target_position, target_orientation)
                self.franka_controller.rrt.update_world()

                # Get current joint positions
                if hasattr(self.franka_controller, 'path_planner_visualizer'):
                    active_joints = self.franka_controller.path_planner_visualizer.get_active_joints_subset()
                    start_pos = active_joints.get_joint_positions()
                else:
                    start_pos = current_joint_positions

                # Set max iterations
                max_iterations = 8000
                self.franka_controller.rrt.set_max_iterations(max_iterations)

                # RETRY LOGIC: Try up to max_retries times
                import time
                for attempt in range(1, max_retries + 1):
                    # Compute RRT path and measure time
                    start_time = time.time()
                    rrt_path = self.franka_controller.rrt.compute_path(start_pos, np.array([]))
                    planning_time = time.time() - start_time

                    if rrt_path is not None and len(rrt_path) > 1:
                        # SUCCESS: Path found!
                        # Calculate actual path length in joint space (sum of waypoint distances)
                        path_length = 0.0
                        for i in range(len(rrt_path) - 1):
                            path_length += np.linalg.norm(rrt_path[i+1] - rrt_path[i])

                        # Calculate optimal path length (straight-line distance in joint space)
                        optimal_path_length = np.linalg.norm(rrt_path[-1] - rrt_path[0])

                        if attempt > 1:
                            print(f"[ENV RRT] Path found for {obj_name} on attempt {attempt}/{max_retries}")

                        return {
                            "success": True,
                            "path_length": path_length,
                            "optimal_path_length": optimal_path_length,
                            "path": rrt_path,
                            "attempts": attempt,
                            "planning_time": planning_time
                        }
                    else:
                        # FAILURE: No path found on this attempt
                        if attempt < max_retries:
                            print(f"[ENV RRT] Attempt {attempt}/{max_retries} failed for {obj_name}, retrying...")
                        else:
                            print(f"[ENV RRT] All {max_retries} attempts failed for {obj_name}")

                # All retries exhausted
                return {"success": False, "reason": "no_path_found_after_retries", "attempts": max_retries}
            else:
                # Fallback: estimate path length using Euclidean distance
                ee_position = self.franka_controller.franka.end_effector.get_world_pose()[0]
                euclidean_distance = np.linalg.norm(target_position - ee_position)

                # Assume path is 1.5x Euclidean distance (rough estimate)
                estimated_path_length = euclidean_distance * 1.5

                return {
                    "success": True,
                    "path_length": estimated_path_length,
                    "path": None,
                    "estimated": True
                }

        except Exception as e:
            print(f"[ENV RRT] Error planning path: {e}")
            return {"success": False, "reason": "exception", "error": str(e)}

    def _execute_pick_place(self, action: int, rrt_path) -> Tuple[bool, str]:
        """
        Execute actual pick-and-place operation.

        Args:
            action: Object index to pick
            rrt_path: RRT path to follow

        Returns:
            Tuple of (success: bool, failure_reason: str)
            - success: True if pick succeeded without collision, False otherwise
            - failure_reason: "collision", "execution_error", or "" if successful
        """
        if not self.execute_picks or self.franka_controller is None:
            # Simulation mode: assume success
            return True, ""

        try:
            # Call the actual pick-and-place execution
            # The franka_controller should have a method like pick_and_place_cube
            if hasattr(self.franka_controller, 'pick_and_place_cube'):
                result = self.franka_controller.pick_and_place_cube(action)

                # Result format: (success: bool, message: str)
                if isinstance(result, tuple) and len(result) >= 2:
                    success, message = result[0], result[1]

                    if not success:
                        # Check if failure was due to collision
                        if "collision" in message.lower() or "obstacle" in message.lower():
                            return False, "collision"
                        else:
                            return False, "execution_error"

                    return True, ""
                else:
                    # Unexpected result format
                    return True, ""
            else:
                # No pick-and-place method available, assume success
                return True, ""

        except Exception as e:
            print(f"[ENV RRT] Error executing pick-place: {e}")
            return False, "execution_error"

    def _get_cube_spacing_for_deployment(self) -> float:
        """
        Get cube spacing that matches deployment script.

        CRITICAL: RRT Isaac Sim training must use same spacing as deployment
        to ensure agent learns correct priorities.

        Returns:
            Cube spacing in meters
        """
        # Match deployment script spacing (lines 685-690 in franka_rrt_physXLidar_depth_camera_rl_standalone_v2.1.py)
        if self.num_cubes <= 4:
            return 0.14  # Gap = 8.85cm
        elif self.num_cubes <= 9:
            return 0.12  # Gap = 6.85cm
        else:
            return 0.10  # Gap = 4.85cm

    def _generate_random_objects(self, use_pygame_style=True):
        """
        Override to use deployment spacing instead of training spacing.

        This ensures RRT Isaac Sim training matches deployment environment.
        """
        # Use FIXED training grid size (e.g., 6x6 = 36 cells)
        grid_size = self.training_grid_size

        self.object_positions = []
        self.object_types = []
        self.object_names = []
        self.obstacle_scores = []

        # OVERRIDE: Use deployment spacing instead of training spacing
        cube_spacing = self._get_cube_spacing_for_deployment()
        grid_center_x = 0.45
        grid_center_y = -0.10
        grid_extent_x = (grid_size - 1) * cube_spacing
        grid_extent_y = (grid_size - 1) * cube_spacing
        start_x = grid_center_x - (grid_extent_x / 2.0)
        start_y = grid_center_y - (grid_extent_y / 2.0)
        random_offset_range = 0.03  # ±3cm random offset

        if use_pygame_style:
            # PyGame-style: Fixed number of objects in random cells of FIXED grid
            total_cells = grid_size * grid_size
            n_objects = min(self.num_cubes, total_cells - 1)  # Reserve 1 cell for EE home position

            # EE home position: bottom-middle cell for any grid size
            ee_home_row = grid_size - 1  # Bottom row
            ee_home_col = grid_size // 2  # Middle column
            ee_home_cell = ee_home_row * grid_size + ee_home_col

            # Get random cells (excluding EE home cell)
            available_cells = [i for i in range(total_cells) if i != ee_home_cell]
            occupied_cells = random.sample(available_cells, n_objects)

            # Generate random obstacles in empty cells
            self.random_obstacle_positions = self._generate_random_obstacles(
                grid_size=grid_size,
                occupied_cells=occupied_cells + [ee_home_cell],
                cube_spacing=cube_spacing,
                grid_center_x=grid_center_x,
                grid_center_y=grid_center_y
            )

            # Place objects in selected cells
            for cell_idx in occupied_cells:
                row = cell_idx // grid_size
                col = cell_idx % grid_size

                # Calculate grid position
                x = start_x + col * cube_spacing
                y = start_y + row * cube_spacing

                # Add small random offset
                x += np.random.uniform(-random_offset_range, random_offset_range)
                y += np.random.uniform(-random_offset_range, random_offset_range)
                z = 0.5  # Fixed height

                self.object_positions.append(np.array([x, y, z]))
                self.object_types.append(0)  # 0 = cube
                self.object_names.append(f"cube_{len(self.object_positions)}")

            self.total_objects = len(self.object_positions)

            # Calculate obstacle scores for all objects
            for i in range(self.total_objects):
                score = self._calculate_obstacle_score_with_unpicked_cubes(
                    self.object_positions[i], i
                )
                self.obstacle_scores.append(score)

    def _get_observation(self, recalculate_obstacles: bool = False) -> np.ndarray:
        """
        Override to calculate distance to actual placement position instead of container center.

        This matches deployment script behavior where cubes are placed in a grid pattern
        inside the container with specific margins.
        """
        obs = np.zeros((self.max_objects, 6), dtype=np.float32)

        # Get robot EE position and container position
        if self.franka_controller:
            self.ee_position = self.franka_controller.franka.end_effector.get_world_pose()[0]
            container_position = self.franka_controller.container.get_world_pose()[0]
            # Get container dimensions from controller
            container_dimensions = getattr(self.franka_controller, 'container_dimensions', np.array([0.48, 0.36, 0.128]))
        else:
            container_position = self.container_position
            container_dimensions = np.array([0.48, 0.36, 0.128])  # Default container dimensions

        ee_position = self.ee_position

        # Calculate how many cubes have been picked (for placement position calculation)
        picked_count = len(self.objects_picked)

        for i in range(self.total_objects):
            obj_pos = self.object_positions[i]

            # 1. Distance to EE (1 value)
            dist_to_ee = np.linalg.norm(obj_pos - ee_position)
            obs[i, 0] = dist_to_ee

            # 2. Distance to container (1 value)
            # OVERRIDE: Calculate distance to actual placement position instead of container center
            # This matches deployment script logic (lines 2305-2333)
            placement_position = calculate_placement_position(
                cube_index=picked_count,  # Next placement slot
                total_cubes=self.num_cubes,
                container_center=container_position,
                container_dimensions=container_dimensions
            )
            dist_to_container = np.linalg.norm(obj_pos - placement_position)
            obs[i, 1] = dist_to_container

            # 3. Obstacle proximity score (1 value)
            # FIX 2: Use consistent obstacle score calculation (includes unpicked cubes)
            if recalculate_obstacles:
                # DYNAMIC: Recalculate obstacle score in real-time
                obstacle_score = self._calculate_obstacle_score_with_unpicked_cubes(obj_pos, i)
                obs[i, 2] = obstacle_score
            else:
                # STATIC: Use pre-calculated score from reset()
                obs[i, 2] = self.obstacle_scores[i]

            # 4. Reachability flag (1 value)
            reachability = self._calculate_reachability(i, dist_to_ee)
            obs[i, 3] = reachability

            # 5. Path clearance score (1 value)
            path_clearance = self._calculate_path_clearance(ee_position, obj_pos)
            obs[i, 4] = path_clearance

            # 6. Picked flag (1 value)
            obs[i, 5] = 1.0 if i in self.objects_picked else 0.0

        return obs.flatten()

    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """Reset environment and clear RRT statistics"""
        obs, info = super().reset(seed=seed, options=options)

        # Add RRT statistics to info (from previous episode)
        if len(self.rrt_planning_times) > 0:
            info["avg_rrt_planning_time"] = np.mean(self.rrt_planning_times)
            info["avg_rrt_path_length"] = np.mean(self.rrt_path_lengths)
            info["total_rrt_path_length"] = np.sum(self.rrt_path_lengths)  # Total actual path length
            info["avg_rrt_optimal_path_length"] = np.mean(self.rrt_optimal_path_lengths) if len(self.rrt_optimal_path_lengths) > 0 else 0.0
            info["total_rrt_optimal_path_length"] = np.sum(self.rrt_optimal_path_lengths) if len(self.rrt_optimal_path_lengths) > 0 else 0.0
            info["rrt_success_rate"] = self.rrt_success_count / (self.rrt_success_count + self.rrt_failure_count) if (self.rrt_success_count + self.rrt_failure_count) > 0 else 0.0
        else:
            info["avg_rrt_planning_time"] = 0.0
            info["avg_rrt_path_length"] = 0.0
            info["total_rrt_path_length"] = 0.0
            info["avg_rrt_optimal_path_length"] = 0.0
            info["total_rrt_optimal_path_length"] = 0.0
            info["rrt_success_rate"] = 0.0

        # Add per-episode failure tracking to info (from previous episode)
        info["episode_rrt_failures"] = self.episode_rrt_failures
        info["episode_pick_failures"] = self.episode_pick_failures
        info["episode_collisions"] = self.episode_collisions
        info["episode_successful_picks"] = self.episode_successful_picks
        info["episode_unreachable_cubes"] = self.episode_unreachable_cubes

        # Reset statistics for new episode
        self.rrt_planning_times = []
        self.rrt_path_lengths = []
        self.rrt_optimal_path_lengths = []
        self.rrt_success_count = 0
        self.rrt_failure_count = 0

        # Reset per-episode counters
        self.episode_rrt_failures = 0
        self.episode_pick_failures = 0
        self.episode_collisions = 0
        self.episode_successful_picks = 0
        self.episode_unreachable_cubes = 0

        # Reset skipped cubes list
        self.cubes_skipped = []

        return obs, info

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        Execute action with RRT planning and proper success tracking.

        CRITICAL FIX: Only mark object as picked if:
        1. RRT successfully finds a collision-free path
        2. Pick execution succeeds without collision

        This overrides the parent class step() to add RRT-specific logic.
        """
        self.current_step += 1

        # Validate action (should never happen with action masking, but keep as safety check)
        if action >= self.total_objects or action in self.objects_picked:
            print(f"[WARNING] Invalid action {action} selected (already picked or out of range)")
            reward = -10.0
            terminated = False
            truncated = self.current_step >= self.max_steps
            obs = self._get_observation()
            info = self._get_info()
            info["invalid_action"] = True
            info["action_mask"] = self.action_masks()
            return obs, reward, terminated, truncated, info

        # Initialize info dict
        info = {}

        # Step 1: Plan RRT path to target object (1 attempt - action mask already verified)
        obj_position = self.object_positions[action]
        obj_name = self.object_names[action]
        rrt_result = self._plan_rrt_path_to_object(obj_position, obj_name, max_retries=1)

        # Step 2: Check if RRT planning succeeded
        if rrt_result["success"]:
            # RRT found a path!
            self.rrt_success_count += 1

            # Track RRT path length (joint space distance) and planning time
            if "path_length" in rrt_result:
                self.rrt_path_lengths.append(rrt_result["path_length"])
            if "optimal_path_length" in rrt_result:
                self.rrt_optimal_path_lengths.append(rrt_result["optimal_path_length"])
            if "planning_time" in rrt_result:
                self.rrt_planning_times.append(rrt_result["planning_time"])

            # Calculate reward based on successful path
            reward = self._calculate_reward(action)

            # Step 3: Execute pick-and-place (if enabled)
            pick_success, failure_reason = self._execute_pick_place(action, rrt_result.get("path"))

            if pick_success:
                # SUCCESS: Both RRT and pick succeeded!
                self.objects_picked.append(action)
                self.episode_successful_picks += 1

                info["pick_success"] = True
                info["rrt_success"] = True
                info["rrt_attempts"] = rrt_result.get("attempts", 1)

            else:
                # FAILURE: RRT succeeded but pick failed (collision or execution error)
                self.episode_pick_failures += 1

                if failure_reason == "collision":
                    self.episode_collisions += 1
                    reward = -20.0  # Heavy penalty for collision during pick
                    info["collision"] = True
                    info["pick_failed_reason"] = "collision"
                else:
                    reward = -15.0  # Penalty for execution error
                    info["pick_failed_reason"] = failure_reason

                info["pick_success"] = False
                info["rrt_success"] = True

        else:
            # FAILURE: RRT failed to find a path
            self.rrt_failure_count += 1
            self.episode_rrt_failures += 1

            # DO NOT mark as picked! The cube may become reachable on next step
            # Action mask will re-evaluate reachability every step

            attempts = rrt_result.get("attempts", 3)
            info["cube_skipped"] = False  # Not skipped, just unreachable this step
            info["skip_reason"] = f"rrt_unreachable_after_{attempts}_attempts"
            print(f"[ENV RRT] Cube {action} unreachable after {attempts} RRT attempts.")

            # Penalty for selecting unreachable object
            reward = -25.0

            info["rrt_success"] = False
            info["rrt_failed_reason"] = rrt_result.get("reason", "unknown")
            info["rrt_attempts"] = attempts
            info["pick_success"] = False

        # Check if episode is done
        # CRITICAL: Only count SUCCESSFULLY picked objects (exclude skipped cubes)
        successfully_picked = len(self.objects_picked) - len(self.cubes_skipped)
        terminated = successfully_picked >= self.num_cubes
        truncated = self.current_step >= self.max_steps

        # EARLY TERMINATION: If no valid actions remain, terminate episode
        # Calculate action mask once and reuse it
        action_mask = self.action_masks()

        if not terminated and not truncated:
            if not action_mask.any():
                # Count unreachable cubes (cubes that were never picked and are now unreachable)
                unpicked_count = self.num_cubes - len(self.objects_picked)
                self.episode_unreachable_cubes = unpicked_count

                print(f"[ENV RRT] No valid actions remaining (all remaining cubes unreachable)")
                print(f"[ENV RRT] Successfully picked: {successfully_picked}/{self.num_cubes}, Unreachable: {unpicked_count}")
                terminated = True
                info["termination_reason"] = "no_valid_actions"

        # Get new observation
        obs = self._get_observation(recalculate_obstacles=self.dynamic_obstacles)
        info["action_mask"] = action_mask  # Reuse cached action mask

        # Bonus for completing all objects
        if terminated:
            time_bonus = max(0, (self.max_steps - self.current_step) * 0.5)
            reward += 20.0 + time_bonus
            info["success"] = True

        # Add RRT statistics to info
        if len(self.rrt_planning_times) > 0:
            info["last_rrt_planning_time"] = self.rrt_planning_times[-1]
        if len(self.rrt_path_lengths) > 0:
            info["last_rrt_path_length"] = self.rrt_path_lengths[-1]

        # Add per-episode failure tracking
        info["episode_rrt_failures"] = self.episode_rrt_failures
        info["episode_pick_failures"] = self.episode_pick_failures
        info["episode_collisions"] = self.episode_collisions
        info["episode_successful_picks"] = self.episode_successful_picks
        info["episode_unreachable_cubes"] = self.episode_unreachable_cubes
        info["episode_cubes_skipped"] = len(self.cubes_skipped)
        info["successfully_picked_count"] = successfully_picked

        # CRITICAL FIX: Add RRT statistics for CURRENT episode when episode ends
        # This ensures the test script can access RRT metrics from the final step() call
        if terminated or truncated:
            if len(self.rrt_planning_times) > 0:
                info["avg_rrt_planning_time"] = np.mean(self.rrt_planning_times)
                info["avg_rrt_path_length"] = np.mean(self.rrt_path_lengths)
                info["total_rrt_path_length"] = np.sum(self.rrt_path_lengths)
                info["avg_rrt_optimal_path_length"] = np.mean(self.rrt_optimal_path_lengths) if len(self.rrt_optimal_path_lengths) > 0 else 0.0
                info["total_rrt_optimal_path_length"] = np.sum(self.rrt_optimal_path_lengths) if len(self.rrt_optimal_path_lengths) > 0 else 0.0
                info["rrt_success_rate"] = self.rrt_success_count / (self.rrt_success_count + self.rrt_failure_count) if (self.rrt_success_count + self.rrt_failure_count) > 0 else 0.0
            else:
                info["avg_rrt_planning_time"] = 0.0
                info["avg_rrt_path_length"] = 0.0
                info["total_rrt_path_length"] = 0.0
                info["avg_rrt_optimal_path_length"] = 0.0
                info["total_rrt_optimal_path_length"] = 0.0
                info["rrt_success_rate"] = 0.0

        return obs, reward, terminated, truncated, info

