_wandb:
    value:
        cli_version: 0.23.1
        e:
            ofzl86ofxqpp2i0c038wkvbyg5jtejyx:
                args:
                    - --grid_size
                    - "3"
                    - --num_cubes
                    - "4"
                    - --timesteps
                    - "50000"
                    - --use_wandb
                codePath: train_rrt_viz_ddqn.py
                codePathLocal: train_rrt_viz_ddqn.py
                cpu_count: 20
                cpu_count_logical: 28
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "1022880641024"
                        used: "716504068096"
                executable: C:\Users\Simulation\AppData\Local\Programs\Python\Python311\python.exe
                gpu: NVIDIA RTX 3500 Ada Generation Laptop GPU
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ada
                      cudaCores: 5120
                      memoryTotal: "12878610432"
                      name: NVIDIA RTX 3500 Ada Generation Laptop GPU
                      uuid: GPU-f3c2eeb3-17e9-4df8-c80a-1c7aa17f7ea8
                host: CRD-SIMULATION5
                memory:
                    total: "33999233024"
                os: Windows-10-10.0.26200-SP0
                program: C:\isaacsim\cobotproject\scripts\Reinforcement Learning\doubleDQN_script\train_rrt_viz_ddqn.py
                python: CPython 3.11.9
                root: C:\isaacsim\cobotproject\scripts\Reinforcement Learning\doubleDQN_script
                startedAt: "2025-12-20T01:53:59.645293Z"
                writerId: ofzl86ofxqpp2i0c038wkvbyg5jtejyx
        m:
            - "1": train/q_std
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": global_step
              "6":
                - 3
              "7": []
            - "1": training/loss
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": training/episode_length_running
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": episode/total_reward
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/action_accuracy
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/learning_rate
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": performance/episode_count
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": training/episode_reward_running
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/td_error
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": rewards/mean
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": performance/overall_accuracy
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": episode/success
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/loss_raw
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": training/epsilon
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": training/q_value
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": training/step_reward
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/q_max
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": rewards/std
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": ddqn/q_target
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": ddqn/q_overestimation
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": rewards/max
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": rewards/min
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": episode/avg_reward_100
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/q_mean
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": train/step_time
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": ddqn/value_estimate
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": episode/total_length
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": episode/success_rate_100
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": ddqn/q_policy
              "5": 2
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.11.9
        t:
            "1":
                - 2
                - 1
                - 3
                - 49
                - 105
            "2":
                - 2
                - 1
                - 3
                - 49
                - 105
            "3":
                - 2
                - 7
                - 13
                - 16
            "4": 3.11.9
            "5": 0.23.1
            "8":
                - 2
                - 3
            "12": 0.23.1
            "13": windows-amd64
batch_size:
    value: 64
epsilon_decay:
    value: 0.995
epsilon_decay_rate:
    value: 2500
epsilon_decay_type:
    value: exponential
epsilon_end:
    value: 0.01
epsilon_start:
    value: 1
gamma:
    value: 0.99
grid_size:
    value: 3
learning_rate:
    value: 0.001
method:
    value: rrt_viz
num_cubes:
    value: 4
target_update_tau:
    value: 0.005
timesteps:
    value: 50000
warmup_steps:
    value: 1000
