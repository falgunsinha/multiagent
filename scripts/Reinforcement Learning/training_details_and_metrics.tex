\section{Training Details}

\subsection{PPO Training}

PPO used the Stable-Baselines3 implementation with MaskablePPO for action masking support. The implementation leveraged vectorized environments for efficient data collection. Policy network used a multi-layer perceptron architecture. Network consisted of two hidden layers with $256$ neurons each. Activation function was ReLU for all hidden layers. Output layer used softmax activation for action probability distribution.

Training process collected experience through environment interaction. Each update cycle gathered $2048$ timesteps of experience. Collected data was split into minibatches for gradient updates. Each update performed $10$ epochs of optimization on the collected data. Training was conducted for $30{,}000$ timesteps. Table~\ref{tab:ppo_params} shows the PPO hyperparameter configuration.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Learning Rate & $3 \times 10^{-4}$ \\
Batch Size & $64$ \\
Steps per Update & $2048$ \\
Epochs per Update & $10$ \\
Discount Factor ($\gamma$) & $0.99$ \\
GAE Lambda ($\lambda$) & $0.95$ \\
Clip Range & $0.2$ \\
Value Function Coefficient & $0.5$ \\
Entropy Coefficient & $0.01$ \\
Max Gradient Norm & $0.5$ \\
Policy Network Architecture & $[256, 256]$ \\
Activation Function & ReLU \\
Optimizer & Adam \\
Total Timesteps & $30{,}000$\\
\hline
\end{tabular}
\caption{PPO Training Hyperparameters}
\label{tab:ppo_params}
\end{table}

\subsection{DDQN Training}

DDQN used a custom PyTorch implementation with separate online and target networks. Online network was updated at each training step. Target network was updated using soft updates with coefficient $\tau = 0.005$ to stabilize learning. Both networks shared the same architecture. Q-network used a multi-layer perceptron with two hidden layers of $256$ neurons each. Activation function was ReLU for hidden layers. The output layer produced Q-values for each action without activation.

Training used experience replay for sample efficiency. A replay buffer stored up to $100{,}000$ transitions. Each training step sampled a mini-batch of $64$ transitions from the buffer. Agent used a warmup period of $1000$ steps before training started. Target network updated every step using soft update mechanism with $\tau = 0.005$. Exploration used epsilon-greedy strategy with exponential decay. Epsilon value started at $1.0$ and decayed exponentially with rate $2500$. Final epsilon reached $0.01$ after sufficient exploration. Total training budget was $30{,}000$ timesteps. Table~\ref{tab:ddqn_params} shows the complete DDQN hyperparameter configuration.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Learning Rate & $1 \times 10^{-3}$ \\
Batch Size & $64$ \\
Discount Factor ($\gamma$) & $0.99$ \\
Replay Buffer Size & $100{,}000$ \\
Warmup Steps & $1000$ \\
Target Update Type & Soft Update \\
Soft Update Coefficient ($\tau$) & $0.005$ \\
Initial Epsilon ($\epsilon_0$) & $1.0$ \\
Final Epsilon ($\epsilon_{min}$) & $0.01$ \\
Epsilon Decay Rate & $2500$ \\
Epsilon Decay Type & Exponential \\
Q-Network Architecture & $[256, 256]$ \\
Activation Function & ReLU \\
Optimizer & Adam \\
Total Timesteps & $30{,}000$ \\
\hline
\end{tabular}
\caption{DDQN Training Hyperparameters}
\label{tab:ddqn_params}
\end{table}

\subsection{GAT-CVD Training}

GAT-CVD used a heterogeneous multi-agent architecture combining DDQN for object selection and MASAC for spatial manipulation. Both agents shared a Graph Attention Network encoder for spatial reasoning. The GAT encoder consisted of $2$ layers with $4$ attention heads per layer. Hidden dimension was $128$ for all layers. Node features included position, type, and object identifiers with dimension $7$. Edge features encoded distance, reachability, and blocking relationships with dimension $3$. Both Agent 1 (DDQN) and Agent 2 (MASAC) were trained simultaneously without freezing either agent.

The shared GAT encoder processed graph-structured observations. Each GAT layer applied multi-head attention with concatenation of attention heads. Dropout rate of $0.1$ was applied for regularization. The encoder output dimension was $128$. DDQN agent used the encoded features to compute Q-values for object selection. MASAC agent used the same encoded features for spatial manipulation policy. Counterfactual Value Decomposition module with hidden dimension $64$ performed credit assignment between the two agents.

Training used a shared replay buffer of size $50{,}000$ transitions. Both agents sampled from the same buffer with batch size $32$. DDQN used epsilon-greedy exploration starting at $\epsilon = 1.0$ and decaying to $0.01$ with rate $0.995$. MASAC used entropy regularization with target entropy $-5.0$ for exploration. All networks used Adam optimizer with learning rate $3 \times 10^{-4}$. Target networks for both agents updated using soft updates with $\tau = 0.005$. Training was conducted for $50{,}000$ timesteps. Table~\ref{tab:gat_cvd_params} shows the GAT-CVD hyperparameter configuration.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
\multicolumn{2}{c}{\textbf{GAT Encoder}} \\
\hline
Node Feature Dimension & $7$ \\
Edge Feature Dimension & $3$ \\
Hidden Dimension & $128$ \\
Output Dimension & $128$ \\
Number of GAT Layers & $2$ \\
Number of Attention Heads & $4$ \\
Dropout Rate & $0.1$ \\
Activation Function & ReLU \\
\hline
\multicolumn{2}{c}{\textbf{DDQN Agent (Object Selection)}} \\
\hline
Learning Rate & $3 \times 10^{-4}$ \\
Initial Epsilon ($\epsilon_0$) & $1.0$ \\
Final Epsilon ($\epsilon_{min}$) & $0.01$ \\
Epsilon Decay Rate & $0.995$ \\
Q-Network Architecture & GAT + $[128]$ \\
\hline
\multicolumn{2}{c}{\textbf{MASAC Agent (Spatial Manipulation)}} \\
\hline
Learning Rate & $3 \times 10^{-4}$ \\
Target Entropy & $-5.0$ \\
Policy Network Architecture & GAT + $[128]$ \\
\hline
\multicolumn{2}{c}{\textbf{CVD Module}} \\
\hline
Hidden Dimension & $64$ \\
Number of Agents & $2$ \\
\hline

