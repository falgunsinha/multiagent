#!/usr/bin/env python3
# train_sb3.py
# ä½¿ç”¨Isaac Lab + Stable-Baselines3 è®­ç»ƒFrankaæŠ“å–ä»»åŠ¡

"""
æ¨èé…ç½®:
- ä½æ˜¾å­˜ (8GB):   --num_envs 2048
- ä¸­ç­‰æ˜¾å­˜ (16GB): --num_envs 4096
- é«˜æ˜¾å­˜ (24GB):   --num_envs 8192
- è¶…é«˜æ˜¾å­˜ (48GB): --num_envs 16384

ä½¿ç”¨æ–¹æ³•:
# è®­ç»ƒï¼ˆ2048ä¸ªå¹¶è¡Œç¯å¢ƒï¼‰
python train_sb3.py --num_envs 2048 --headless --timesteps 10000000

# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
python train_sb3.py --num_envs 16 --test --checkpoint models/franka_pickplace.zip
"""

import argparse
import os
from datetime import datetime

# Isaac Labå¯¼å…¥
try:
    from isaaclab.app import AppLauncher
except ImportError:
    from omni.isaac.lab.app import AppLauncher

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="è®­ç»ƒFrankaæŠ“å–ä»»åŠ¡")
parser.add_argument("--num_envs", type=int, default=2048, help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
parser.add_argument("--task", type=str, default="FrankaPickPlace", help="ä»»åŠ¡åç§°")
parser.add_argument("--headless", action="store_true", help="æ— å¤´æ¨¡å¼")
parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼")
parser.add_argument("--checkpoint", type=str, default=None, help="æ£€æŸ¥ç‚¹è·¯å¾„")
parser.add_argument("--total_timesteps", type=int, default=10000000, help="è®­ç»ƒæ€»æ­¥æ•°")
args_cli = parser.parse_args()

# å¯åŠ¨Isaac Labåº”ç”¨
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# åœ¨å¯åŠ¨åå¯¼å…¥å…¶ä»–æ¨¡å—
import torch
import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback
from stable_baselines3.common.logger import configure

from franka_pickplace_env import FrankaPickPlaceEnv
from franka_pickplace_env_cfg import FrankaPickPlaceEnvCfg


# =================================================================
# è‡ªå®šä¹‰å›è°ƒ - æ˜¾ç¤ºè¯¦ç»†å¥–åŠ±ä¿¡æ¯
# =================================================================
class DetailedRewardCallback(BaseCallback):
    """
    è‡ªå®šä¹‰å›è°ƒï¼šæ˜¾ç¤ºè¯¦ç»†çš„å¥–åŠ±ç»Ÿè®¡ï¼ŒåŒ…æ‹¬æ¯ä¸ªå¥–åŠ±åˆ†é‡
    """
    def __init__(self, check_freq=1, verbose=0):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.rollout_count = 0
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨ä¸€æ¬¡ï¼Œå¿…é¡»å®ç°"""
        return True
        
    def _on_rollout_end(self) -> None:
        """åœ¨æ¯ä¸ªrolloutç»“æŸæ—¶æ‰“å°è¯¦ç»†ç»Ÿè®¡"""
        self.rollout_count += 1
        
        # åªåœ¨æŒ‡å®šé¢‘ç‡æ‰“å°
        if self.rollout_count % self.check_freq != 0:
            return
            
        # è·å–åŸºç¡€ç¯å¢ƒ
        base_env = self.training_env.envs[0].env if hasattr(self.training_env, 'envs') else self.training_env.env
        
        # è·å–å¥–åŠ±ç®¡ç†å™¨çš„ç»Ÿè®¡ä¿¡æ¯
        if hasattr(base_env, 'reward_manager'):
            reward_manager = base_env.reward_manager
            
            print("\n" + "=" * 80)
            print(f"ğŸ“Š Rollout #{self.rollout_count} å¥–åŠ±è¯¦æƒ… (æ€»æ­¥æ•°: {self.num_timesteps:,})")
            print("=" * 80)
            
            # æ˜¾ç¤ºå„ä¸ªå¥–åŠ±åˆ†é‡çš„ç»Ÿè®¡
            if hasattr(reward_manager, '_term_names') and hasattr(reward_manager, '_term_cfgs'):
                print("\nå„å¥–åŠ±åˆ†é‡ç»Ÿè®¡:")
                print("-" * 80)
                
                # è·å–æœ€è¿‘çš„å¥–åŠ±å€¼
                for idx, term_name in enumerate(reward_manager._term_names):
                    # _term_cfgs æ˜¯åˆ—è¡¨ï¼Œä½¿ç”¨ç´¢å¼•è®¿é—®
                    cfg = reward_manager._term_cfgs[idx]
                    weight = cfg.weight
                    
                    # å°è¯•è·å–è¯¥å¥–åŠ±é¡¹çš„ç»Ÿè®¡ä¿¡æ¯
                    if hasattr(reward_manager, '_episode_sums') and term_name in reward_manager._episode_sums:
                        term_sum = reward_manager._episode_sums[term_name]
                        print(f"  {term_name:20s}: æƒé‡={weight:6.1f}, ç´¯è®¡={term_sum.mean().item():+.4f}")
                
            # æ˜¾ç¤ºSB3çš„episodeç»Ÿè®¡
            if len(self.model.ep_info_buffer) > 0:
                ep_rewards = [info['r'] for info in self.model.ep_info_buffer]
                ep_lengths = [info['l'] for info in self.model.ep_info_buffer]
                
                print("\n" + "-" * 80)
                print(f"å®Œæˆçš„episodes: {len(ep_rewards)}")
                print(f"å¹³å‡episodeå¥–åŠ±: {np.mean(ep_rewards):+.4f} Â± {np.std(ep_rewards):.4f}")
                print(f"æœ€å°/æœ€å¤§å¥–åŠ±: {np.min(ep_rewards):+.4f} / {np.max(ep_rewards):+.4f}")
                print(f"å¹³å‡episodeé•¿åº¦: {np.mean(ep_lengths):.1f} æ­¥")
                
                if len(ep_rewards) >= 10:
                    recent = ep_rewards[-10:]
                    print(f"æœ€è¿‘10ä¸ªepisodes: {np.mean(recent):+.4f} Â± {np.std(recent):.4f}")
            
            print("=" * 80 + "\n")
        
        return True


class DetailedLoggingCallback(BaseCallback):
    """è®°å½•æ¯ä¸ªrolloutçš„å¹³å‡å¥–åŠ±å’Œå…¶ä»–ç»Ÿè®¡ä¿¡æ¯"""
    
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        
    def _on_step(self) -> bool:
        # è·å–å½“å‰çš„å¥–åŠ±
        if "reward" in self.locals:
            rewards = self.locals["rewards"]
            # è®°å½•å¹³å‡å¥–åŠ±
            if len(rewards) > 0:
                mean_reward = np.mean(rewards)
                self.logger.record("rollout/mean_reward_step", mean_reward)
        
        return True
    
    def _on_rollout_end(self) -> bool:
        """åœ¨æ¯ä¸ªrolloutç»“æŸæ—¶è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        # è®°å½•rollout bufferä¸­çš„ä¿¡æ¯
        if hasattr(self.model, 'rollout_buffer'):
            buffer = self.model.rollout_buffer
            if buffer.full:
                mean_reward = np.mean(buffer.rewards)
                self.logger.record("rollout/mean_reward", mean_reward)
                self.logger.record("rollout/mean_value", np.mean(buffer.values))
        
        return True


# =================================================================
# Isaac Labç¯å¢ƒåŒ…è£…å™¨ for Stable-Baselines3
# =================================================================
from stable_baselines3.common.vec_env import VecEnv


class IsaacLabVecEnvWrapper(VecEnv):
    """å°†Isaac Labå‘é‡åŒ–ç¯å¢ƒåŒ…è£…ä¸ºSB3çš„VecEnvæ¥å£"""
    
    def __init__(self, env):
        self.env = env
        self.num_envs = env.num_envs
        
        # ä½¿ç”¨ policy è§‚å¯Ÿç©ºé—´ï¼Œç§»é™¤æ‰¹æ¬¡ç»´åº¦
        original_obs_space = env.observation_space["policy"]
        if len(original_obs_space.shape) == 2:
            # å½¢çŠ¶æ˜¯ (num_envs, obs_dim)ï¼Œå–å•ä¸ªç¯å¢ƒçš„ç»´åº¦
            obs_shape = (original_obs_space.shape[1],)
        else:
            # å½¢çŠ¶å·²ç»æ˜¯ (obs_dim,)
            obs_shape = original_obs_space.shape
        
        observation_space = gym.spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=obs_shape,
            dtype=np.float32
        )
        
        # ä¿®å¤åŠ¨ä½œç©ºé—´è¾¹ç•Œï¼ˆSB3 éœ€è¦æœ‰é™è¾¹ç•Œï¼‰ï¼Œç§»é™¤æ‰¹æ¬¡ç»´åº¦
        original_action_space = env.action_space
        if len(original_action_space.shape) == 2:
            # å½¢çŠ¶æ˜¯ (num_envs, action_dim)ï¼Œå–å•ä¸ªç¯å¢ƒçš„ç»´åº¦
            action_shape = (original_action_space.shape[1],)
        else:
            # å½¢çŠ¶å·²ç»æ˜¯ (action_dim,)
            action_shape = original_action_space.shape
        
        action_space = gym.spaces.Box(
            low=-1.0,
            high=1.0,
            shape=action_shape,
            dtype=np.float32
        )
        
        # åˆå§‹åŒ–VecEnv
        super().__init__(self.num_envs, observation_space, action_space)
        
        self.reset_infos = [{} for _ in range(self.num_envs)]
    
    def _to_numpy(self, tensor):
        """å°†torch tensorè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆè‡ªåŠ¨å¤„ç†GPU/CPUï¼‰"""
        if isinstance(tensor, torch.Tensor):
            return tensor.cpu().numpy()
        return np.array(tensor)
    
    def reset(self):
        obs_dict, info = self.env.reset()
        obs = self._to_numpy(obs_dict["policy"])
        return obs
    
    def step_async(self, actions):
        """å¼‚æ­¥æ‰§è¡ŒåŠ¨ä½œï¼ˆIsaac Labæ˜¯åŒæ­¥çš„ï¼Œæ‰€ä»¥ç›´æ¥å­˜å‚¨ï¼‰"""
        self.actions = actions
    
    def step_wait(self):
        """ç­‰å¾…stepå®Œæˆå¹¶è¿”å›ç»“æœ"""
        # è½¬æ¢actionä¸ºtorch tensorï¼Œç¡®ä¿æ˜¯2D
        if isinstance(self.actions, np.ndarray):
            actions = torch.from_numpy(self.actions).float()
        else:
            actions = torch.as_tensor(self.actions, dtype=torch.float32)
        
        # ç¡®ä¿å½¢çŠ¶æ­£ç¡®ï¼šåº”è¯¥æ˜¯ (num_envs, action_dim)
        if actions.shape[0] != self.num_envs:
            raise ValueError(f"Action batch size mismatch: expected {self.num_envs}, got {actions.shape[0]}")
        
        actions = actions.to(self.env.device)
        
        obs_dict, rewards, terminated, truncated, infos = self.env.step(actions)
        
        # è½¬æ¢æ‰€æœ‰è¾“å‡ºä¸ºnumpyæ•°ç»„
        obs = self._to_numpy(obs_dict["policy"])
        rewards = self._to_numpy(rewards)
        
        # å…ˆè½¬æ¢ä¸ºnumpyå†è¿›è¡Œé€»è¾‘è¿ç®—
        terminated_np = self._to_numpy(terminated)
        truncated_np = self._to_numpy(truncated)
        dones = np.logical_or(terminated_np, truncated_np)
        
        # SB3æœŸæœ›infosæ˜¯ä¸€ä¸ªå­—å…¸åˆ—è¡¨
        if not isinstance(infos, list):
            infos = [{} for _ in range(self.num_envs)]
        
        return obs, rewards, dones, infos
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        self.env.close()
    
    def env_is_wrapped(self, wrapper_class, indices=None):
        """æ£€æŸ¥ç¯å¢ƒæ˜¯å¦è¢«åŒ…è£…"""
        return [False] * self.num_envs
    
    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):
        """è°ƒç”¨ç¯å¢ƒæ–¹æ³•"""
        return [getattr(self.env, method_name)(*method_args, **method_kwargs)]
    
    def get_attr(self, attr_name, indices=None):
        """è·å–ç¯å¢ƒå±æ€§"""
        return [getattr(self.env, attr_name)]
    
    def set_attr(self, attr_name, value, indices=None):
        """è®¾ç½®ç¯å¢ƒå±æ€§"""
        setattr(self.env, attr_name, value)
    
    def seed(self, seed=None):
        """è®¾ç½®éšæœºç§å­"""
        if seed is not None:
            self.env.seed(seed)
        return [seed] * self.num_envs


# =================================================================
# è®­ç»ƒå‡½æ•°
# =================================================================
def train():
    """ä½¿ç”¨Stable-Baselines3è®­ç»ƒ"""
    print("=" * 60)
    print("ğŸš€ ä½¿ç”¨Stable-Baselines3è®­ç»ƒï¼ˆGPUåŠ é€Ÿï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºç¯å¢ƒé…ç½®
    env_cfg = FrankaPickPlaceEnvCfg()
    env_cfg.scene.num_envs = args_cli.num_envs
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"ğŸ“¦ åˆ›å»º {args_cli.num_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ...")
    print(f"ğŸ”§ é…ç½®ç¡®è®¤: env_cfg.scene.num_envs = {env_cfg.scene.num_envs}")
    
    # æ£€æŸ¥GPUæ˜¾å­˜
    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        total_mem_gb = gpu_props.total_memory / 1024**3
        print(f"ğŸ® GPU: {gpu_props.name}")
        print(f"ğŸ’¾ æ€»æ˜¾å­˜: {total_mem_gb:.2f} GB")
        allocated_gb = torch.cuda.memory_allocated(0) / 1024**3
        print(f"ğŸ“Š å·²åˆ†é…: {allocated_gb:.2f} GB")
    
    base_env = FrankaPickPlaceEnv(cfg=env_cfg)
    
    # æ£€æŸ¥ç¯å¢ƒåˆ›å»ºåçš„æ˜¾å­˜
    if torch.cuda.is_available():
        allocated_gb = torch.cuda.memory_allocated(0) / 1024**3
        reserved_gb = torch.cuda.memory_reserved(0) / 1024**3
        print(f"ğŸ’¾ ç¯å¢ƒåˆ›å»ºåæ˜¾å­˜: åˆ†é…={allocated_gb:.2f}GB, ä¿ç•™={reserved_gb:.2f}GB")
    
    print(f"âœ… å®é™…åˆ›å»ºçš„ç¯å¢ƒæ•°: {base_env.num_envs}")
    
    # æ£€æŸ¥ç¯å¢ƒæ•°æ˜¯å¦åŒ¹é…
    if base_env.num_envs != args_cli.num_envs:
        print("\n" + "="*70)
        print("âš ï¸  è­¦å‘Š: ç¯å¢ƒæ•°ä¸åŒ¹é…!")
        print(f"   è¯·æ±‚ç¯å¢ƒæ•°: {args_cli.num_envs}")
        print(f"   å®é™…ç¯å¢ƒæ•°: {base_env.num_envs}")
        print(f"   å·®å¼‚: {args_cli.num_envs - base_env.num_envs} ä¸ªç¯å¢ƒæœªåˆ›å»º")
        print("\n   å¯èƒ½åŸå› :")
        print("   1. Isaac Lab å†…éƒ¨æœ‰æœ€å¤§ç¯å¢ƒæ•°é™åˆ¶")
        print("   2. é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼è¦†ç›–äº†å‘½ä»¤è¡Œå‚æ•°")
        print("   3. æ˜¾å­˜ä¸è¶³ï¼Œç³»ç»Ÿè‡ªåŠ¨é™çº§")
        print("="*70 + "\n")
        
        user_input = input("æ˜¯å¦ç»§ç»­è®­ç»ƒ? (y/n): ")
        if user_input.lower() != 'y':
            print("è®­ç»ƒå·²å–æ¶ˆ")
            return
    else:
        print(f"âœ… ç¯å¢ƒæ•°åŒ¹é…! æˆåŠŸåˆ›å»ºäº†æ‰€æœ‰ {base_env.num_envs} ä¸ªç¯å¢ƒ\n")
    
    env = IsaacLabVecEnvWrapper(base_env)
    
    print(f"ğŸ” è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"ğŸ” åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"ğŸ” æœ€ç»ˆå¹¶è¡Œç¯å¢ƒæ•°: {env.num_envs}")
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"./logs/sb3_{timestamp}"
    model_dir = f"./models"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)
    
    # é…ç½®å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=10000000 // args_cli.num_envs,  # æ¯1kwæ­¥ä¿å­˜ä¸€æ¬¡
        save_path=model_dir,
        name_prefix="franka_pickplace",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )
    
    # æ·»åŠ è¯¦ç»†å¥–åŠ±æ—¥å¿—å›è°ƒ
    reward_callback = DetailedRewardCallback(check_freq=1, verbose=1)
    
    from stable_baselines3.common.callbacks import CallbackList
    callbacks = CallbackList([checkpoint_callback, reward_callback])
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    if args_cli.checkpoint and os.path.exists(args_cli.checkpoint):
        print(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹åŠ è½½: {args_cli.checkpoint}")
        model = PPO.load(
            args_cli.checkpoint,
            env=env,
            device="cuda" if torch.cuda.is_available() else "cpu",
        )
    else:
        print("ğŸ†• åˆ›å»ºæ–°æ¨¡å‹")
        # æ ¹æ®ç¯å¢ƒæ•°åŠ¨æ€è°ƒæ•´batch_size
        num_envs = args_cli.num_envs
        # batch_sizeåº”è¯¥æ˜¯num_envsçš„å€æ•°ï¼Œä¸”è¶³å¤Ÿå¤§ä»¥å……åˆ†åˆ©ç”¨GPU
        if num_envs >= 8192:
            batch_size = 8192
            n_steps = 16
        elif num_envs >= 4096:
            batch_size = 4096
            n_steps = 32
        else:
            batch_size = 2048
            n_steps = 32
        
        print(f"ğŸ“Š è®­ç»ƒé…ç½®: num_envs={num_envs}, n_steps={n_steps}, batch_size={batch_size}")
        print(f"   æ¯æ¬¡æ›´æ–°æ”¶é›†: {num_envs * n_steps} æ­¥")
        print(f"   æ¯æ¬¡æ›´æ–°æ¢¯åº¦æ­¥æ•°: {(num_envs * n_steps) // batch_size * 10} æ­¥")
        
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=10,  # æ¯æ¬¡æ›´æ–°è®­ç»ƒ10ä¸ªepoch
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,  # é¼“åŠ±æ¢ç´¢
            vf_coef=0.5,  # ä»·å€¼å‡½æ•°æŸå¤±æƒé‡
            max_grad_norm=0.5,  # æ¢¯åº¦è£å‰ª
            verbose=1,
            tensorboard_log=log_dir,
            device="cuda" if torch.cuda.is_available() else "cpu",
            policy_kwargs=dict(
                net_arch=dict(pi=[256, 128, 64], vf=[256, 128, 64]),
                activation_fn=torch.nn.ELU,
            ),
        )
    
    # é…ç½®æ—¥å¿—
    new_logger = configure(log_dir, ["stdout", "csv", "tensorboard"])
    model.set_logger(new_logger)
    
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ ({args_cli.total_timesteps} æ€»æ­¥æ•°)")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {model_dir}")
    print(f"ğŸ“Š æ—¥å¿—ä¿å­˜åˆ°: {log_dir}")
    print(f"ğŸ“ˆ TensorBoard: tensorboard --logdir={log_dir}")
    print("=" * 60)
    
    # æ˜¾ç¤ºè®­ç»ƒå¼€å§‹å‰çš„æ˜¾å­˜
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ è®­ç»ƒå‰æ˜¾å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.2f}GB")
        print("â³ è®­ç»ƒå¼€å§‹åæ˜¾å­˜ä¼šå¢é•¿ï¼Œè¯·ç”¨ nvidia-smi ç›‘æ§\n")
    
    # å¼€å§‹è®­ç»ƒ
    model.learn(
        total_timesteps=args_cli.total_timesteps,
        callback=callbacks,
        progress_bar=True,
        log_interval=1,  # æ¯ä¸ªrolloutåè®°å½•æ—¥å¿—
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = f"{model_dir}/franka_pickplace_final.zip"
    model.save(final_model_path)
    print(f"âœ… è®­ç»ƒå®Œæˆ! æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")
    
    env.close()


# =================================================================
# æµ‹è¯•å‡½æ•°
# =================================================================
def test():
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if not args_cli.checkpoint:
        print("âŒ æµ‹è¯•æ¨¡å¼éœ€è¦æŒ‡å®š --checkpoint å‚æ•°")
        return
        
    if not os.path.exists(args_cli.checkpoint):
        print(f"âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {args_cli.checkpoint}")
        return
    
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•æ¨¡å¼")
    print("=" * 60)
    
    # åˆ›å»ºç¯å¢ƒ
    env_cfg = FrankaPickPlaceEnvCfg()
    env_cfg.scene.num_envs = args_cli.num_envs
    base_env = FrankaPickPlaceEnv(cfg=env_cfg)
    env = IsaacLabVecEnvWrapper(base_env)
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {args_cli.checkpoint}")
    model = PPO.load(args_cli.checkpoint, device="cuda" if torch.cuda.is_available() else "cpu")
    
    # è¿è¡Œæµ‹è¯•
    print("â–¶ï¸ å¼€å§‹æµ‹è¯•...")
    obs = env.reset()  # VecEnv.reset() åªè¿”å›è§‚å¯Ÿ
    episode_rewards = []
    current_rewards = np.zeros(env.num_envs)
    
    for step in range(1000):  # æµ‹è¯•1000æ­¥
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, dones, infos = env.step(action)
        current_rewards += reward
        
        # è®°å½•å®Œæˆçš„episode
        if dones.any():
            episode_rewards.extend(current_rewards[dones].tolist())
            current_rewards[dones] = 0
            
        if step % 100 == 0:
            print(f"æ­¥æ•°: {step}, å¹³å‡å¥–åŠ±: {np.mean(current_rewards):.2f}")
    
    if episode_rewards:
        print(f"\nâœ… æµ‹è¯•å®Œæˆ!")
        print(f"å®Œæˆçš„episodeæ•°: {len(episode_rewards)}")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
        print(f"æœ€å¤§å¥–åŠ±: {np.max(episode_rewards):.2f}")
        print(f"æœ€å°å¥–åŠ±: {np.min(episode_rewards):.2f}")
    
    env.close()


# =================================================================
# ä¸»å‡½æ•°
# =================================================================
def main():
    print("=" * 60)
    print("ğŸ¤– Isaac Lab + Stable-Baselines3 - FrankaæŠ“å–ä»»åŠ¡")
    print("=" * 60)
    print(f"ç¯å¢ƒæ•°é‡: {args_cli.num_envs}")
    print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
    print("=" * 60)
    
    if args_cli.test:
        test()
    else:
        train()
    
    simulation_app.close()


if __name__ == "__main__":
    main()
