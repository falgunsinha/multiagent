#!/usr/bin/env python3
"""
å¥–åŠ±è®¡ç®—éªŒè¯è„šæœ¬
ç›®çš„ï¼šéªŒè¯ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š
1. å¥–åŠ±å‡½æ•°æ˜¯å¦çœŸçš„è¢« RewardManager è°ƒç”¨ï¼Ÿ
2. å¥–åŠ±å€¼æ˜¯å¦åœ¨å˜åŒ–ï¼Œè¿˜æ˜¯ä¸€ç›´ä¸º0ï¼Ÿ
3. å¥–åŠ±æ˜¯å¦æ­£ç¡®ä¼ é€’ç»™äº† SB3 çš„ PPO ç®—æ³•ï¼Ÿ
"""

import argparse
from isaaclab.app import AppLauncher

parser = argparse.ArgumentParser(description="éªŒè¯å¥–åŠ±è®¡ç®—")
parser.add_argument("--num_envs", type=int, default=128, help="å¹¶è¡Œç¯å¢ƒæ•°")
args_cli = parser.parse_args()

app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

import torch
from franka_pickplace_env import FrankaPickPlaceEnv
from franka_pickplace_env_cfg import FrankaPickPlaceEnvCfg

def main():
    print("=" * 70)
    print("ğŸ” å¥–åŠ±è®¡ç®—éªŒè¯æµ‹è¯•")
    print("=" * 70)
    print("\nç›®æ ‡ï¼šéªŒè¯å¥–åŠ±å‡½æ•°æ˜¯å¦è¢«è°ƒç”¨ï¼Œä»¥åŠå¥–åŠ±å€¼æ˜¯å¦å˜åŒ–\n")
    
    # åˆ›å»ºç¯å¢ƒ
    env_cfg = FrankaPickPlaceEnvCfg()
    env_cfg.scene.num_envs = args_cli.num_envs
    
    print(f"ğŸ“¦ åˆ›å»º {args_cli.num_envs} ä¸ªç¯å¢ƒ...")
    env = FrankaPickPlaceEnv(cfg=env_cfg)
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"   è§‚æµ‹ç»´åº¦: {env.observation_manager.group_obs_dim['policy']}")
    print(f"   åŠ¨ä½œç»´åº¦: {env.action_manager.total_action_dim}")
    print(f"\nğŸ¯ å¼€å§‹æµ‹è¯• - è¿è¡Œ1000æ­¥ï¼Œæ¯100æ­¥ç»Ÿè®¡ä¸€æ¬¡å¥–åŠ±\n")
    print("-" * 70)
    
    # é‡ç½®ç¯å¢ƒ
    obs, _ = env.reset()
    
    total_rewards = []
    step_rewards = []
    
    for step in range(1000):
        # ç”ŸæˆéšæœºåŠ¨ä½œ
        actions = torch.randn(env.num_envs, env.action_manager.total_action_dim, device=env.device)
        actions = torch.clamp(actions, -1.0, 1.0)
        
        # æ‰§è¡Œæ­¥éª¤
        obs, rewards, terminated, truncated, info = env.step(actions)
        
        # æ”¶é›†å¥–åŠ±ç»Ÿè®¡
        step_rewards.append(rewards.mean().item())
        total_rewards.extend(rewards.cpu().numpy().tolist())
        
        # æ¯100æ­¥æ‰“å°ä¸€æ¬¡ç»Ÿè®¡
        if (step + 1) % 100 == 0:
            recent_mean = sum(step_rewards[-100:]) / 100
            recent_std = torch.tensor(step_rewards[-100:]).std().item()
            print(f"æ­¥æ•° {step+1:4d}: æœ€è¿‘100æ­¥å¹³å‡å¥–åŠ±={recent_mean:+.6f} Â± {recent_std:.6f}")
    
    print("\n" + "=" * 70)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print("=" * 70)
    
    import numpy as np
    total_rewards = np.array(total_rewards)
    
    print(f"\næ€»æ­¥æ•°: {len(step_rewards)}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(total_rewards):+.6f}")
    print(f"æ ‡å‡†å·®: {np.std(total_rewards):.6f}")
    print(f"æœ€å°å¥–åŠ±: {np.min(total_rewards):+.6f}")
    print(f"æœ€å¤§å¥–åŠ±: {np.max(total_rewards):+.6f}")
    print(f"å¥–åŠ±ä¸º0çš„æ¯”ä¾‹: {(total_rewards == 0).sum() / len(total_rewards) * 100:.1f}%")
    
    # å…³é”®è¯Šæ–­
    print("\n" + "=" * 70)
    print("ğŸ” è¯Šæ–­ç»“æœ")
    print("=" * 70)
    
    if not hasattr(env, '_reward_call_count'):
        print("\nâŒ é—®é¢˜1: å¥–åŠ±å‡½æ•°ä»æœªè¢«è°ƒç”¨!")
        print("   å¯èƒ½åŸå› : RewardManageré…ç½®é”™è¯¯ï¼Œæˆ–ç¯å¢ƒæ²¡æœ‰æ­£ç¡®åˆå§‹åŒ–")
    else:
        print(f"\nâœ… å¥–åŠ±å‡½æ•°è¢«è°ƒç”¨äº† {env._reward_call_count} æ¬¡")
    
    if np.all(total_rewards == total_rewards[0]):
        print("\nâŒ é—®é¢˜2: æ‰€æœ‰å¥–åŠ±å€¼å®Œå…¨ç›¸åŒ (constant)!")
        print(f"   å›ºå®šå€¼: {total_rewards[0]:.6f}")
        print("   å¯èƒ½åŸå› : å¥–åŠ±å‡½æ•°å†…éƒ¨é€»è¾‘æœ‰é—®é¢˜ï¼Œè¿”å›äº†å¸¸é‡")
    elif np.std(total_rewards) < 1e-6:
        print("\nâš ï¸  é—®é¢˜2: å¥–åŠ±å€¼å‡ ä¹ä¸å˜åŒ– (std < 1e-6)!")
        print("   å¯èƒ½åŸå› : å¥–åŠ±å‡½æ•°å¯¹çŠ¶æ€å˜åŒ–ä¸æ•æ„Ÿ")
    else:
        print(f"\nâœ… å¥–åŠ±å€¼åœ¨å˜åŒ– (std={np.std(total_rewards):.6f})")
    
    if (total_rewards == 0).sum() / len(total_rewards) > 0.95:
        print("\nâŒ é—®é¢˜3: è¶…è¿‡95%çš„å¥–åŠ±ä¸º0!")
        print("   å¯èƒ½åŸå› : å¥–åŠ±æ¡ä»¶å¤ªè‹›åˆ»ï¼Œæ™ºèƒ½ä½“å‡ ä¹æ— æ³•è·å¾—æ­£å¥–åŠ±")
    else:
        print(f"\nâœ… å¥–åŠ±åˆ†å¸ƒæ­£å¸¸ ({(total_rewards != 0).sum() / len(total_rewards) * 100:.1f}%éé›¶)")
    
    print("\n" + "=" * 70)
    print("âœ… éªŒè¯æµ‹è¯•å®Œæˆ")
    print("=" * 70)
    
    # æŸ¥çœ‹è°ƒè¯•è¾“å‡ºçš„æç¤º
    print("\nğŸ’¡ æç¤º: å‘ä¸Šæ»šåŠ¨æŸ¥çœ‹ '[REWARD FUNC CALLED]' å¼€å¤´çš„è°ƒè¯•è¾“å‡º")
    print("   å¦‚æœçœ‹åˆ°è¿™äº›è¾“å‡ºï¼Œè¯´æ˜å¥–åŠ±å‡½æ•°ç¡®å®åœ¨è¿è¡Œ")
    print("   å¦‚æœæ²¡æœ‰ï¼Œè¯´æ˜ RewardManager æ ¹æœ¬æ²¡æœ‰è°ƒç”¨æˆ‘ä»¬çš„å‡½æ•°\n")
    
    env.close()

if __name__ == "__main__":
    main()
    simulation_app.close()
