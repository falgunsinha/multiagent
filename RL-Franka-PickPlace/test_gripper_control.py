#!/usr/bin/env python3
"""
å¤¹çˆªæ§åˆ¶éªŒè¯è„šæœ¬
ç”¨äºå¯è§†åŒ–æµ‹è¯•å¤¹çˆªæ˜¯å¦çœŸçš„åœ¨å“åº”åŠ¨ä½œæŒ‡ä»¤
"""

import argparse
from isaaclab.app import AppLauncher

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="æµ‹è¯•Frankaå¤¹çˆªæ§åˆ¶")
parser.add_argument("--num_envs", type=int, default=4, help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
args_cli = parser.parse_args()

# å¯åŠ¨Isaac Labåº”ç”¨ï¼ˆéheadlessæ¨¡å¼ï¼Œéœ€è¦å¯è§†åŒ–ï¼‰
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

import torch
from franka_pickplace_env import FrankaPickPlaceEnv
from franka_pickplace_env_cfg import FrankaPickPlaceEnvCfg

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”§ Frankaå¤¹çˆªæ§åˆ¶éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºç¯å¢ƒé…ç½®
    env_cfg = FrankaPickPlaceEnvCfg()
    env_cfg.scene.num_envs = args_cli.num_envs
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"\nğŸ“¦ åˆ›å»º {args_cli.num_envs} ä¸ªå¯è§†åŒ–ç¯å¢ƒ...")
    env = FrankaPickPlaceEnv(cfg=env_cfg)
    
    print("\nâœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")
    print(f"   è§‚æµ‹ç©ºé—´ç»´åº¦: {env.observation_manager.group_obs_dim['policy']}")
    print(f"   åŠ¨ä½œç©ºé—´ç»´åº¦: {env.action_manager.total_action_dim}")
    
    # é‡ç½®ç¯å¢ƒ
    obs, _ = env.reset()
    
    print("\nğŸ® å¼€å§‹æµ‹è¯•åºåˆ—...")
    print("   æµ‹è¯•1: æ‰€æœ‰åŠ¨ä½œä¸º0 (å¤¹çˆªåº”ä¿æŒæ‰“å¼€)")
    print("   æµ‹è¯•2: å¤¹çˆªåŠ¨ä½œ=+1 (å¤¹çˆªåº”å°è¯•é—­åˆ)")
    print("   æµ‹è¯•3: å¤¹çˆªåŠ¨ä½œ=-1 (å¤¹çˆªåº”æ‰“å¼€)")
    print("\nè¯·ä»”ç»†è§‚å¯Ÿä»¿çœŸçª—å£ä¸­çš„æœºå™¨äººå¤¹çˆªï¼\n")
    
    # æµ‹è¯•åºåˆ—
    test_steps = 100
    
    # é˜¶æ®µ1ï¼šæ‰€æœ‰åŠ¨ä½œä¸º0ï¼ˆ50æ­¥ï¼‰
    print("ğŸ”¹ é˜¶æ®µ1: é›¶åŠ¨ä½œæµ‹è¯•...")
    for i in range(50):
        actions = torch.zeros(env.num_envs, env.action_manager.total_action_dim, device=env.device)
        obs, reward, terminated, truncated, info = env.step(actions)
        if i % 10 == 0:
            print(f"  Step {i}: å¤¹çˆªåŠ¨ä½œå€¼ = 0.0")
    
    # é˜¶æ®µ2ï¼šå¤¹çˆªé—­åˆæŒ‡ä»¤ï¼ˆ50æ­¥ï¼‰
    print("\nğŸ”¹ é˜¶æ®µ2: å¤¹çˆªé—­åˆæµ‹è¯• (action[7] = +1.0)...")
    for i in range(50):
        actions = torch.zeros(env.num_envs, env.action_manager.total_action_dim, device=env.device)
        # è®¾ç½®ç¬¬8ä¸ªåŠ¨ä½œï¼ˆç´¢å¼•7ï¼‰ä¸ºæ­£å€¼ï¼ŒæŒ‡ç¤ºé—­åˆ
        if env.action_manager.total_action_dim >= 8:
            actions[:, 7] = 1.0
        elif env.action_manager.total_action_dim >= 9:
            actions[:, 7] = 1.0  # æˆ–è€… actions[:, 8] = 1.0ï¼Œå–å†³äºé…ç½®
        obs, reward, terminated, truncated, info = env.step(actions)
        if i % 10 == 0:
            print(f"  Step {i+50}: å¤¹çˆªåŠ¨ä½œå€¼ = +1.0 (é—­åˆæŒ‡ä»¤)")
    
    # é˜¶æ®µ3ï¼šå¤¹çˆªæ‰“å¼€æŒ‡ä»¤ï¼ˆ50æ­¥ï¼‰
    print("\nğŸ”¹ é˜¶æ®µ3: å¤¹çˆªæ‰“å¼€æµ‹è¯• (action[7] = -1.0)...")
    for i in range(50):
        actions = torch.zeros(env.num_envs, env.action_manager.total_action_dim, device=env.device)
        if env.action_manager.total_action_dim >= 8:
            actions[:, 7] = -1.0
        elif env.action_manager.total_action_dim >= 9:
            actions[:, 7] = -1.0
        obs, reward, terminated, truncated, info = env.step(actions)
        if i % 10 == 0:
            print(f"  Step {i+100}: å¤¹çˆªåŠ¨ä½œå€¼ = -1.0 (æ‰“å¼€æŒ‡ä»¤)")
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("\nè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š")
    print("1. åœ¨é˜¶æ®µ2ä¸­ï¼Œæ‚¨æ˜¯å¦çœ‹åˆ°å¤¹çˆªçš„ä¸¤ä¸ªæŒ‡ç‰‡æœ‰å‘å†…é—­åˆçš„åŠ¨ä½œï¼Ÿ")
    print("2. åœ¨é˜¶æ®µ3ä¸­ï¼Œå¤¹çˆªæ˜¯å¦é‡æ–°æ‰“å¼€ï¼Ÿ")
    print("\nå¦‚æœç­”æ¡ˆæ˜¯'æ˜¯'ï¼Œè¯´æ˜å¤¹çˆªæ§åˆ¶æ­£å¸¸ã€‚")
    print("å¦‚æœç­”æ¡ˆæ˜¯'å¦'ï¼Œè¯´æ˜åŠ¨ä½œç©ºé—´é…ç½®æœ‰é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥ActionsCfgã€‚")
    print("=" * 60)
    
    # å…³é—­ç¯å¢ƒ
    env.close()

if __name__ == "__main__":
    main()
    simulation_app.close()
