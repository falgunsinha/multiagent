# franka_pickplace_env_cfg.py
# Isaac Labç¯å¢ƒé…ç½® - FrankaæŠ“å–ä»»åŠ¡
# æ”¯æŒæ•°åƒä¸ªå¹¶è¡Œç¯å¢ƒGPUè®­ç»ƒ

import math
import torch

from isaaclab.utils import configclass
from isaaclab.envs import ManagerBasedRLEnvCfg
from isaaclab.scene import InteractiveSceneCfg
from isaaclab.sim import SimulationCfg
from isaaclab.managers import SceneEntityCfg, EventTermCfg, RewardTermCfg, TerminationTermCfg, ObservationTermCfg, ActionTermCfg, ObservationGroupCfg
from isaaclab.envs.mdp.actions import JointPositionActionCfg
from isaaclab.assets import ArticulationCfg, AssetBaseCfg, RigidObjectCfg
from isaaclab.sensors import ContactSensorCfg
import isaaclab.sim as sim_utils
from isaaclab_assets import FRANKA_PANDA_CFG



##
# Sceneé…ç½®
##

@configclass
class FrankaPickPlaceSceneCfg(InteractiveSceneCfg):
    """åœºæ™¯é…ç½®ï¼šåŒ…å«æœºå™¨äººã€æ–¹å—ã€åœ°é¢ç­‰"""
    
    # åœ°é¢
    ground = AssetBaseCfg(
        prim_path="/World/ground",
        spawn=sim_utils.GroundPlaneCfg(),
    )
    
    # Frankaæœºå™¨äºº
    robot: ArticulationCfg = FRANKA_PANDA_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
    
    # æ–¹å—
    cube: RigidObjectCfg = RigidObjectCfg(
        prim_path="{ENV_REGEX_NS}/Cube",
        spawn=sim_utils.CuboidCfg(
            size=(0.05, 0.05, 0.05),  # 5cmç«‹æ–¹ä½“
            rigid_props=sim_utils.RigidBodyPropertiesCfg(
                solver_position_iteration_count=16,
                solver_velocity_iteration_count=1,
                max_angular_velocity=1000.0,
                max_linear_velocity=1000.0,
                disable_gravity=False,
            ),
            mass_props=sim_utils.MassPropertiesCfg(mass=0.05),
            collision_props=sim_utils.CollisionPropertiesCfg(),
            visual_material=sim_utils.PreviewSurfaceCfg(
                diffuse_color=(0.8, 0.2, 0.2),  # çº¢è‰²
                metallic=0.2,
            ),
        ),
        init_state=RigidObjectCfg.InitialStateCfg(pos=(0.4, 0.0, 0.025)),
    )
    



##
# MDPè®¾ç½®
##

def get_observations(env):
    """è·å–æ‰€æœ‰è§‚æµ‹å¹¶æ‹¼æ¥"""
    # å…³èŠ‚ä½ç½® (7ç»´)
    joint_pos = env.scene["robot"].data.joint_pos[:, :7]
    # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® (3ç»´)
    ee_pos = env.scene["robot"].data.body_pos_w[:, -1, :]
    # æ–¹å—ä½ç½® (3ç»´)
    cube_pos = env.scene["cube"].data.root_pos_w
    
    # -- START: å…³é”®ä¿®æ”¹åœ¨è¿™é‡Œ --
    # ä»ç¯å¢ƒçš„é…ç½®(cfg)ä¸­è·å–ç›®æ ‡ä½ç½®åˆ—è¡¨ï¼Œå¹¶è½¬æ¢ä¸ºä¸€ä¸ªä¸ç¯å¢ƒæ•°é‡å’Œè®¾å¤‡åŒ¹é…çš„å¼ é‡
    # æ³¨æ„ï¼šè¿™ä¼šåœ¨æ¯ä¸€æ­¥éƒ½åˆ›å»ºå¼ é‡ï¼Œå¯¹äºæ€§èƒ½æœ‰è½»å¾®å½±å“ã€‚
    # æ›´ä¼˜åŒ–çš„æ–¹æ³•æ˜¯åœ¨ç¯å¢ƒçš„ __init__ ä¸­åˆ›å»ºä¸€æ¬¡ self.target_posã€‚ä½†è¿™ä¸ªæ–¹æ³•å¯ä»¥ç›´æ¥è§£å†³å½“å‰é—®é¢˜ã€‚
    target_pos = torch.tensor(env.cfg.target_position, dtype=torch.float32, device=env.device).repeat(env.num_envs, 1)
    # -- END: å…³é”®ä¿®æ”¹åœ¨è¿™é‡Œ --
    # è·ç¦» (1ç»´)
    ee_to_cube = torch.norm(cube_pos - ee_pos, dim=-1, keepdim=True)
    cube_to_target = torch.norm(cube_pos - target_pos, dim=-1, keepdim=True)
    
    # è·å–å¤¹æŒçŠ¶æ€ (1ç»´) - å®‰å…¨è·å–ï¼Œå¤„ç†åˆå§‹åŒ–é˜¶æ®µ
    if hasattr(env, 'cube_is_grasped') and env.cube_is_grasped is not None:
        cube_is_grasped = env.cube_is_grasped.unsqueeze(-1)
    else:
        # åˆå§‹åŒ–é˜¶æ®µé»˜è®¤æœªæŠ“å–
        cube_is_grasped = torch.zeros((env.num_envs, 1), dtype=torch.bool, device=env.device)
    
    # æ ¹æ®æ˜¯å¦æŠ“å–æ¥å†³å®šå…³æ³¨å“ªä¸ªè·ç¦»
    distance = torch.where(cube_is_grasped, cube_to_target, ee_to_cube)
    
    # æ‹¼æ¥æ‰€æœ‰è§‚æµ‹ (7+3+3+3+1+1=18ç»´)
    return torch.cat([joint_pos, ee_pos, cube_pos, target_pos, distance, cube_is_grasped.float()], dim=-1)

@configclass 
class ObservationsCfg:
    """è§‚æµ‹é…ç½®"""
    
    @configclass
    class PolicyCfg(ObservationGroupCfg):
        """Policyç½‘ç»œçš„è§‚æµ‹"""
        concatenate_terms = True
        enable_corruption = False
        
        @configclass
        class AllObsCfg(ObservationTermCfg):
            func = get_observations
        
        all_obs: AllObsCfg = AllObsCfg()
    
    policy: PolicyCfg = PolicyCfg()


@configclass
class ActionsCfg:
    """åŠ¨ä½œé…ç½®ï¼š8ä¸ªè‡ªç”±åº¦ï¼ˆ7ä¸ªå…³èŠ‚ + 1ä¸ªå¤¹çˆªï¼‰çš„è¿ç»­æ§åˆ¶"""
    # ä½¿ç”¨åŸå§‹çš„JointPositionActionCfgä½†æ˜ç¡®æŒ‡å®šæ‰€æœ‰9ä¸ªå…³èŠ‚ï¼ˆ7è‡‚+2æŒ‡ï¼‰
    # è®©ç¯å¢ƒä»£ç åœ¨_pre_physics_stepä¸­å¤„ç†8ç»´è¾“å…¥åˆ°9ä¸ªå…³èŠ‚çš„æ˜ å°„
    joint_pos: ActionTermCfg = JointPositionActionCfg(
        asset_name="robot",
        joint_names=["panda_joint[1-7]", "panda_finger_joint.*"],  # 7ä¸ªæ‰‹è‡‚å…³èŠ‚ + 2ä¸ªå¤¹çˆªå…³èŠ‚
        scale=0.5,
        use_default_offset=True
    )


# å®šä¹‰æ˜¾å¼çš„å¥–åŠ±å‡½æ•°ï¼Œé¿å…lamb daé—­åŒ…é—®é¢˜
def reward_distance_to_cube(env) -> torch.Tensor:
    """æ¥è¿‘æ–¹å—å¥–åŠ± - ç®€åŒ–ç‰ˆï¼šè·ç¦»è¶Šè¿‘å¥–åŠ±è¶Šé«˜"""
    if not hasattr(env, 'cube_is_grasped') or not hasattr(env, 'ee_to_cube_dist'):
        return torch.zeros(env.num_envs, device=env.device)
    not_grasped = (~env.cube_is_grasped).float()
    # ä½¿ç”¨ (1 - distance) ç¡®ä¿æ­£å¥–åŠ±
    # è·ç¦»0.5m: (1-0.5)=0.5, è·ç¦»0.1m: (1-0.1)=0.9
    reward = (1.0 - torch.clamp(env.ee_to_cube_dist, 0, 1.0))
    final_reward = reward * not_grasped
    
    # ğŸ” è°ƒè¯•è¾“å‡º - éªŒè¯å¥–åŠ±è®¡ç®—æ˜¯å¦è¢«è°ƒç”¨
    if not hasattr(env, '_reward_call_count'):
        env._reward_call_count = 0
    env._reward_call_count += 1
    if env._reward_call_count % 500 == 1:  # ç¬¬1æ¬¡å’Œæ¯500æ¬¡æ‰“å°
        print(f"\n[REWARD FUNC CALLED] distance_to_cube:")
        print(f"  Final reward: mean={final_reward.mean().item():.4f}, min={final_reward.min().item():.4f}, max={final_reward.max().item():.4f}")
        print(f"  Raw distance (ee_to_cube_dist): mean={env.ee_to_cube_dist.mean().item():.4f}, min={env.ee_to_cube_dist.min().item():.4f}, max={env.ee_to_cube_dist.max().item():.4f}")
        print(f"  Clamped distance: mean={torch.clamp(env.ee_to_cube_dist, 0, 1.0).mean().item():.4f}")
        print(f"  Reward before mask: mean={reward.mean().item():.4f}, min={reward.min().item():.4f}, max={reward.max().item():.4f}")
        print(f"  Not grasped mask: {not_grasped.sum().item()}/{env.num_envs} envs")
    return final_reward

def reward_distance_to_target(env) -> torch.Tensor:
    """æ¬è¿åˆ°ç›®æ ‡å¥–åŠ± - ç®€åŒ–ç‰ˆ"""
    if not hasattr(env, 'cube_is_grasped') or not hasattr(env, 'cube_to_target_dist'):
        return torch.zeros(env.num_envs, device=env.device)
    grasped = env.cube_is_grasped.float()
    reward = (1.0 - torch.clamp(env.cube_to_target_dist, 0, 1.0))
    return reward * grasped

def reward_attempt_grasp(env) -> torch.Tensor:
    """å°è¯•æŠ“å–å¼•å¯¼å¥–åŠ± - å½“æ™ºèƒ½ä½“åœ¨æ­£ç¡®ä½ç½®å°è¯•é—­åˆå¤¹çˆªæ—¶ç»™äºˆå¥–åŠ±"""
    if not hasattr(env, 'is_attempting_grasp'):
        return torch.zeros(env.num_envs, device=env.device)
    rewards = env.is_attempting_grasp.float()
    
    # ğŸ” è°ƒè¯•è¾“å‡º
    if hasattr(env, '_reward_call_count') and env._reward_call_count % 500 == 1:
        active = rewards.sum().item()
        if active > 0:
            print(f"[REWARD FUNC CALLED] attempt_grasp: {int(active)}/{env.num_envs} envs attempting")
    return rewards

def reward_grasp_success(env) -> torch.Tensor:
    """æŠ“å–æˆåŠŸé‡Œç¨‹ç¢‘å¥–åŠ± - åªåœ¨åˆšæŠ“å–çš„é‚£ä¸€æ­¥ç»™äºˆ"""
    if not hasattr(env, 'just_grasped'):
        return torch.zeros(env.num_envs, device=env.device)
    rewards = env.just_grasped.float()
    
    # ğŸ” è°ƒè¯•è¾“å‡º - æˆåŠŸäº‹ä»¶æ€»æ˜¯æ‰“å°
    success_count = rewards.sum().item()
    if success_count > 0:
        print(f"\nğŸ‰ [REWARD FUNC CALLED] grasp_success: {int(success_count)} envs just grasped!\n")
    return rewards

def reward_place_success(env) -> torch.Tensor:
    """æ”¾ç½®æˆåŠŸé‡Œç¨‹ç¢‘å¥–åŠ± - åªåœ¨åˆšæ”¾ç½®æˆåŠŸçš„é‚£ä¸€æ­¥ç»™äºˆ"""
    if not hasattr(env, 'just_placed'):
        return torch.zeros(env.num_envs, device=env.device)
    return env.just_placed.float()

def penalty_action(env) -> torch.Tensor:
    """åŠ¨ä½œå¤§å°æƒ©ç½š - é¿å…è¿‡åº¦å‰§çƒˆçš„åŠ¨ä½œ"""
    if not hasattr(env, 'actions'):
        return torch.zeros(env.num_envs, device=env.device)
    return -torch.sum(env.actions**2, dim=-1) * 0.01  # æ·»åŠ 0.01ç³»æ•°ï¼Œé¿å…æƒ©ç½šè¿‡é‡

@configclass
class RewardsCfg:
    """å¥–åŠ±é…ç½® - å¢åŠ å¼•å¯¼å¥–åŠ±è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜"""
    
    # è·ç¦»å¥–åŠ± - æä¾›æŒç»­å¼•å¯¼ä¿¡å·
    distance_to_cube = RewardTermCfg(func=reward_distance_to_cube, weight=0.5)
    distance_to_target = RewardTermCfg(func=reward_distance_to_target, weight=1.5)
    
    # æ–°å¢ï¼šå°è¯•æŠ“å–å¼•å¯¼å¥–åŠ± - è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜çš„å…³é”®ï¼
    # è¿™ä¼šæ˜ç¡®å‘Šè¯‰æ™ºèƒ½ä½“"åœ¨æ–¹å—é™„è¿‘é—­åˆå¤¹çˆªæ˜¯æ­£ç¡®çš„"
    attempt_grasp = RewardTermCfg(func=reward_attempt_grasp, weight=5.0)
    
    # é‡Œç¨‹ç¢‘å¥–åŠ± - ä¸»è¦å¥–åŠ±æ¥æºï¼ˆé€‚å½“é™ä½æƒé‡ï¼Œå› ä¸ºç°åœ¨æœ‰å¼•å¯¼äº†ï¼‰
    grasp_success = RewardTermCfg(func=reward_grasp_success, weight=20.0)
    place_success = RewardTermCfg(func=reward_place_success, weight=40.0)
    
    # åŠ¨ä½œæƒ©ç½š - é¼“åŠ±å¹³æ»‘æ§åˆ¶
    action_penalty = RewardTermCfg(func=penalty_action, weight=1.0)  # æƒé‡å·²åœ¨å‡½æ•°å†…éƒ¨å¤„ç†


@configclass
class TerminationsCfg:
    """ç»ˆæ­¢æ¡ä»¶é…ç½®"""
    
    # è¶…æ—¶
    time_out = TerminationTermCfg(func=lambda env: env.episode_length_buf >= env.max_episode_length, time_out=True)
    
    # æˆåŠŸå®Œæˆ
    success = TerminationTermCfg(func=lambda env: env.task_success)


##
# ç¯å¢ƒé…ç½®
##

@configclass
class FrankaPickPlaceEnvCfg(ManagerBasedRLEnvCfg):
    """FrankaæŠ“å–ä»»åŠ¡ç¯å¢ƒé…ç½®"""
    
    # åœºæ™¯è®¾ç½®ï¼ˆnum_envsä¼šåœ¨train_sb3.pyä¸­åŠ¨æ€è®¾ç½®ï¼‰
    scene: FrankaPickPlaceSceneCfg = FrankaPickPlaceSceneCfg(num_envs=4096, env_spacing=2.0)
    
    # åŸºç¡€è®¾ç½®
    observations: ObservationsCfg = ObservationsCfg()
    actions: ActionsCfg = ActionsCfg()
    rewards: RewardsCfg = RewardsCfg()
    terminations: TerminationsCfg = TerminationsCfg()
    
    # ä»¿çœŸè®¾ç½®
    sim: SimulationCfg = SimulationCfg(
        dt=1.0 / 60.0,  # 60Hz
        render_interval=2,  # æ¯2ä¸ªç‰©ç†æ­¥æ¸²æŸ“ä¸€æ¬¡
        physics_material=sim_utils.RigidBodyMaterialCfg(
            friction_combine_mode="multiply",
            restitution_combine_mode="multiply",
            static_friction=1.0,
            dynamic_friction=1.0,
        ),
    )
    
    # Episodeè®¾ç½®
    episode_length_s = 10.0  # 10ç§’
    decimation = 2  # æ§åˆ¶é¢‘ç‡ = 60Hz / 2 = 30Hz
    
    # åŠ¨ä½œç¼©æ”¾
    action_scale = 0.5
    
    # ç›®æ ‡ä½ç½®ï¼ˆç±³ï¼‰
    target_position = [0.5, 0.2, 0.0]
    
    # æ–¹å—åˆå§‹ä½ç½®èŒƒå›´
    cube_spawn_x_range = [0.3, 0.5]
    cube_spawn_y_range = [-0.2, 0.2]
    cube_spawn_height = 0.025  # æ–¹å—ä¸€åŠé«˜åº¦
    
    # ä»»åŠ¡å‚æ•°
    grasp_distance_threshold = 0.06  # è§¦å‘æŠ“å–çš„è·ç¦»é˜ˆå€¼
    drop_distance_threshold = 0.2    # åˆ¤æ–­æ‰è½çš„è·ç¦»é˜ˆå€¼
    target_success_threshold = 0.05  # åˆ¤æ–­æ”¾ç½®æˆåŠŸçš„è·ç¦»é˜ˆå€¼
    
    # å¤¹çˆªä½ç½®
    gripper_open_pos = 0.04
    gripper_closed_pos = 0.0
    
    # èµ„äº§é…ç½®
    arm_joint_ids = list(range(7))
    gripper_joint_ids = [7, 8]
    ee_body_name = "panda_hand"
    
    # åŠ¨ä½œç©ºé—´é…ç½®ï¼ˆç”¨äºç¯å¢ƒå†…éƒ¨ï¼‰
    num_actions = 8  # 7ä¸ªå…³èŠ‚ + 1ä¸ªå¤¹çˆª
    action_scale_internal = 0.1  # å†…éƒ¨åŠ¨ä½œç¼©æ”¾
