#!/usr/bin/env python3
# train_isaaclab.py
# ä½¿ç”¨Isaac Lab + Stable-Baselines3 è®­ç»ƒFrankaæŠ“å–ä»»åŠ¡



import argparse
import os
from datetime import datetime

# Isaac Labå¯¼å…¥
try:
    from isaaclab.app import AppLauncher
except ImportError:
    from omni.isaac.lab.app import AppLauncher

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description="è®­ç»ƒFrankaæŠ“å–ä»»åŠ¡")
parser.add_argument("--num_envs", type=int, default=2048, help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
parser.add_argument("--task", type=str, default="FrankaPickPlace", help="ä»»åŠ¡åç§°")
parser.add_argument("--headless", action="store_true", help="æ— å¤´æ¨¡å¼")
parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼")
parser.add_argument("--checkpoint", type=str, default=None, help="æ£€æŸ¥ç‚¹è·¯å¾„")
parser.add_argument("--timesteps", type=int, default=10000000, help="è®­ç»ƒæ€»æ­¥æ•°")
args_cli = parser.parse_args()

# å¯åŠ¨Isaac Labåº”ç”¨
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# åœ¨å¯åŠ¨åå¯¼å…¥å…¶ä»–æ¨¡å—
import torch
import gymnasium as gym
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecNormalize
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback

try:
    from isaaclab.envs import ManagerBasedRLEnv, ManagerBasedRLEnvCfg
    from isaaclab.envs.mdp.actions import JointPositionActionCfg
except ImportError:
    from omni.isaac.lab.envs import ManagerBasedRLEnv, ManagerBasedRLEnvCfg
    from omni.isaac.lab.envs.mdp.actions import JointPositionActionCfg

from franka_pickplace_env import FrankaPickPlaceEnv
from franka_pickplace_env_cfg import FrankaPickPlaceEnvCfg


# =================================================================
# Isaac Labç¯å¢ƒåŒ…è£…å™¨ for Stable-Baselines3
# =================================================================
class IsaacLabWrapper(gym.Wrapper):
    """å°†Isaac Labç¯å¢ƒåŒ…è£…ä¸ºæ ‡å‡†Gymnasiumæ¥å£"""
    
    def __init__(self, env):
        super().__init__(env)
        self.observation_space = env.observation_space["policy"]
        self.action_space = env.action_space
        
    def reset(self, **kwargs):
        obs_dict, info = self.env.reset(**kwargs)
        return obs_dict["policy"], info
        
    def step(self, action):
        obs_dict, reward, terminated, truncated, info = self.env.step(action)
        return obs_dict["policy"], reward, terminated, truncated, info


# =================================================================
# è®­ç»ƒå‡½æ•°ï¼ˆä½¿ç”¨Stable-Baselines3ï¼‰
# =================================================================
def train_with_sb3():
    """ç”Ÿæˆrl-gamesè®­ç»ƒé…ç½®"""
    return {
        "params": {
            "seed": 42,
            "algo": {"name": "a2c_continuous"},
            "model": {"name": "continuous_a2c_logstd"},
            "network": {
                # ä½¿ç”¨æœ€ç®€å•çš„å…±äº«ç½‘ç»œé…ç½®
                "name": "actor_critic",
                "separate": False, # æ¢å¤ä¸º Falseï¼Œä½¿ç”¨å…±äº«ç½‘ç»œ
                "space": {
                    "continuous": {
                        "mu_activation": "None", "sigma_activation": "None",
                        "mu_init": {"name": "default"},
                        "sigma_init": {"name": "const_initializer", "val": 0.0},
                        "fixed_sigma": True,
                    }
                },
                "mlp": {
                    "units": [256, 128, 64], "activation": "elu", "d2rl": False,
                    "initializer": {"name": "default"},
                },
            },
            "config": {
                "name": "FrankaPickPlace", "env_name": "rlgpu", "ppo": True,
                "mixed_precision": False, 
                "normalize_input": True,
                # vvvvvv è¿™æ˜¯è§£å†³æ‰€æœ‰é—®é¢˜çš„æ ¸å¿ƒ vvvvvv
                "normalize_value": False, # ç›´æ¥ç¦ç”¨ä»·å€¼å‡½æ•°å½’ä¸€åŒ–ï¼Œé¿å…æ‰€æœ‰é—®é¢˜
                # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                "value_bootstrap": True, "num_actors": num_envs,
                "reward_shaper": {"scale_value": 0.01}, "normalize_advantage": True,
                "gamma": 0.99, "tau": 0.95, "learning_rate": 3e-4,
                "lr_schedule": "adaptive", "kl_threshold": 0.016,
                "max_epochs": max_iterations, "save_best_after": 100,
                "save_frequency": 50, "print_stats": True, "grad_norm": 1.0,
                "entropy_coef": 0.0, "e_clip": 0.2, "horizon_length": 16,
                "minibatch_size": 8192, "mini_epochs": 8, "critic_coef": 4,
                "clip_value": True, "train_dir": checkpoint_dir,
            },
        }
    }

# =================================================================
# <<< é‡œåº•æŠ½è–ªæ–¹æ¡ˆ PART 2: æ¢å¤æœ€ç®€å•çš„ç¯å¢ƒåŒ…è£…å™¨ >>>
# =================================================================
class IsaacLabVecEnvWrapper(vecenv.IVecEnv):
    """
    ä¸€ä¸ªç®€å•çš„åŒ…è£…å™¨ï¼Œå°†Isaac Labç¯å¢ƒé€‚é…ä¸ºrl-gamesæ ¼å¼ã€‚
    å®ƒåªè¿”å›ä¸€ä¸ªæ‰å¹³çš„è§‚å¯Ÿå¼ é‡ã€‚
    """
    
    def __init__(self, env):
        self.env = env
        self.num_envs = env.num_envs
        obs_space = env.observation_space["policy"]
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=obs_space.shape[1:],  # ä½¿ç”¨ obs_space çš„å½¢çŠ¶
            dtype=np.float32
        )
        self.action_space = env.action_space
        self.num_agents = self.num_envs
        
    def step(self, actions):
        obs_dict, rewards, dones, truncated, info = self.env.step(actions)
        # åªè¿”å›ç­–ç•¥éœ€è¦çš„å¼ é‡
        return obs_dict["policy"], rewards, dones, info
        
    def reset(self, **kwargs):
        obs_dict, _ = self.env.reset(**kwargs)
        # åªè¿”å›ç­–ç•¥éœ€è¦çš„å¼ é‡
        return obs_dict["policy"]
        
    def get_number_of_agents(self):
        return self.num_envs
        
    def get_env_info(self):
        # è¿”å›ç®€å•çš„ Box ç©ºé—´ä¿¡æ¯
        return {
            "observation_space": self.observation_space,
            "action_space": self.action_space,
            "agents": self.num_envs,
        }

def main():
    print("=" * 60)
    print("ğŸ¤– Isaac Lab - FrankaæŠ“å–ä»»åŠ¡è®­ç»ƒ")
    print(f"ç¯å¢ƒæ•°é‡: {args_cli.num_envs}")
    print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
    print("=" * 60)
    
    
    simulation_app.close()

if __name__ == "__main__":
    main()

