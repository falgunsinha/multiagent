"""
RLä¸€ä½“åŒ–è„šæœ¬ - è®­ç»ƒã€æµ‹è¯•ã€è¿è¡Œå…¨æµç¨‹
åŒ…å«ä¸‰ç§æ¨¡å¼ï¼š
1. è®­ç»ƒæ¨¡å¼ (train): è®­ç»ƒRLæ¨¡å‹
2. æµ‹è¯•æ¨¡å¼ (test): æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
3. è¿è¡Œæ¨¡å¼ (run): ä½¿ç”¨RLæ¨¡å‹è¿è¡Œä»»åŠ¡
"""

import os
import argparse
import numpy as np

# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–åŒ…"""
    required_packages = {
        'gymnasium': 'gymnasium>=0.29.0',
        'stable_baselines3': 'stable-baselines3>=2.0.0',
        'torch': 'torch>=2.0.0',
    }
    
    missing = []
    for pkg_name, pkg_version in required_packages.items():
        try:
            __import__(pkg_name)
        except ImportError:
            missing.append(pkg_version)
    
    if missing:
        print("=" * 60)
        print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…:")
        for pkg in missing:
            print(f"   - {pkg}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print("   pip install -r requirements_rl.txt")
        print("=" * 60)
        return False
    return True


# ============================================================================
# æ¨¡å¼1: è®­ç»ƒæ¨¡å¼
# ============================================================================

def train_model(args):
    """è®­ç»ƒRLæ¨¡å‹"""
    from stable_baselines3 import PPO, SAC
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
    from stable_baselines3.common.callbacks import CheckpointCallback
    from stable_baselines3.common.monitor import Monitor
    
    print("=" * 60)
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - ç®—æ³•: {args.algorithm.upper()}")
    print("=" * 60)
    print(f"è®­ç»ƒæ­¥æ•°: {args.timesteps}")
    print(f"æ–¹å—æ•°é‡: {args.cube_num}")
    print(f"æ— å¤´æ¨¡å¼: {args.headless}")
    print("=" * 60)
    
    # âœ… å…ˆåˆå§‹åŒ– Isaac Sim
    from isaacsim import SimulationApp
    simulation_app = SimulationApp({"headless": args.headless})
    
    # âœ… å†å¯¼å…¥ä¾èµ– Isaac Sim çš„æ¨¡å—
    from class_taskEnv import ArmPickPlaceRLEnv
    
    # åˆ›å»ºç¯å¢ƒ
    def make_env():
        env = ArmPickPlaceRLEnv(
            headless=args.headless, 
            cube_num=args.cube_num,
            simulation_app=simulation_app
        )
        return Monitor(env)
    
    dummy_env = DummyVecEnv([make_env])
    vecnorm_path_guess = None
    if args.resume_from:
        if args.resume_vecnormalize:
            vecnorm_path_guess = args.resume_vecnormalize
        else:
            base_name = os.path.splitext(args.resume_from)[0]
            # å…¼å®¹é»˜è®¤ä¿å­˜è·¯å¾„: ./models/vec_normalize_{algorithm}.pkl
            candidate = base_name + "_vecnormalize.pkl"
            if os.path.exists(candidate):
                vecnorm_path_guess = candidate
            else:
                default_vec = f"./models/vec_normalize_{args.algorithm}.pkl"
                if os.path.exists(default_vec):
                    vecnorm_path_guess = default_vec

    if vecnorm_path_guess and os.path.exists(vecnorm_path_guess):
        print(f"åŠ è½½ VecNormalize çŠ¶æ€: {vecnorm_path_guess}")
        env = VecNormalize.load(vecnorm_path_guess, dummy_env)
        env.training = True
        env.norm_reward = True
    else:
        if args.resume_from and not vecnorm_path_guess:
            print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„ VecNormalize æ–‡ä»¶, å°†é‡æ–°åˆå§‹åŒ–å½’ä¸€åŒ–ç»Ÿè®¡ã€‚")
        env = VecNormalize(dummy_env)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs("./models", exist_ok=True)
    os.makedirs("./logs", exist_ok=True)
    
    # è®¾ç½®checkpointå›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path="./models/",
        name_prefix=f"{args.algorithm}_armpickplace"
    )
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆä¼˜åŒ–è®­ç»ƒé€Ÿåº¦ï¼‰
    if args.algorithm != "ppo":
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {args.algorithm}")

    if args.resume_from:
        if not os.path.exists(args.resume_from):
            raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°æŒ‡å®šçš„ checkpoint: {args.resume_from}")
        print(f"åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ: {args.resume_from}")
        model = PPO.load(args.resume_from, env=env)
    else:
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=4096,  
            batch_size=256,  
            n_epochs=10,  
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.02,
            verbose=1,
            tensorboard_log="./logs/",
            policy_kwargs=dict(
                net_arch=[dict(pi=[256, 256], vf=[256, 256])]  #
            )
        )
    
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("æç¤º: ä½¿ç”¨ 'tensorboard --logdir ./logs/' æŸ¥çœ‹è®­ç»ƒè¿›åº¦")
    print("-" * 60)
    
    # è®­ç»ƒ
    model.learn(
        total_timesteps=args.timesteps,
        callback=checkpoint_callback,
        progress_bar=True,
        reset_num_timesteps=not bool(args.resume_from)
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = f"./models/{args.algorithm}_armpickplace_final"
    model.save(final_model_path)
    env.save(f"./models/vec_normalize_{args.algorithm}.pkl")
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}.zip")
    print(f"VecNormalizeå·²ä¿å­˜åˆ°: ./models/vec_normalize_{args.algorithm}.pkl")
    print("\nä¸‹ä¸€æ­¥:")
    print(f"  æµ‹è¯•æ¨¡å‹: python {__file__} test --model {final_model_path}.zip")
    print(f"  è¿è¡Œæ¨¡å‹: python {__file__} run --model {final_model_path}.zip")
    print("=" * 60)
    
    env.close()


# ============================================================================
# æ¨¡å¼2: æµ‹è¯•æ¨¡å¼
# ============================================================================

def test_model(args):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    from stable_baselines3 import PPO, SAC
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
    
    print("=" * 60)
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹ - {args.model}")
    print("=" * 60)
    
    # âœ… å…ˆåˆå§‹åŒ– Isaac Sim
    from isaacsim import SimulationApp
    simulation_app = SimulationApp({"headless": args.headless})
    
    # âœ… å†å¯¼å…¥ä¾èµ– Isaac Sim çš„æ¨¡å—
    from class_taskEnv import ArmPickPlaceRLEnv
    
    # åˆ›å»ºç¯å¢ƒ
    env = ArmPickPlaceRLEnv(
        render_mode="human",
        headless=args.headless, 
        cube_num=args.cube_num,
        simulation_app=simulation_app
    )
    
    # åŠ è½½VecNormalizeï¼ˆå¦‚æœæä¾›ï¼‰
    if args.vec_normalize:
        vec_env = DummyVecEnv([lambda: env])
        vec_env = VecNormalize.load(args.vec_normalize, vec_env)
        vec_env.training = False
        vec_env.norm_reward = False
    else:
        vec_env = None
    
    # åŠ è½½æ¨¡å‹
    if args.algorithm == "ppo":
        model = PPO.load(args.model)
    elif args.algorithm == "sac":
        model = SAC.load(args.model)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {args.algorithm}")
    
    print(f"æµ‹è¯• {args.episodes} ä¸ªå›åˆ...")
    print("-" * 60)
    
    # ç»Ÿè®¡ä¿¡æ¯
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(args.episodes):
        if vec_env:
            obs = vec_env.reset()
        else:
            obs, info = env.reset()
        
        episode_reward = 0
        episode_length = 0
        done = False
        
        print(f"\nğŸ“ Episode {episode + 1}/{args.episodes}")
        
        while not done:
            # é¢„æµ‹åŠ¨ä½œ
            action, _ = model.predict(obs, deterministic=True)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            if vec_env:
                obs, reward, done, info = vec_env.step(action)
            else:
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
            
            episode_reward += reward
            episode_length += 1
            
        
        # è®°å½•ç»Ÿè®¡
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"æµ‹è¯•å›åˆæ•°: {args.episodes}")
    print(f"æˆåŠŸç‡: {success_count}/{args.episodes} ({100*success_count/args.episodes:.1f}%)")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}")
    print(f"æœ€é«˜å¥–åŠ±: {np.max(episode_rewards):.2f}")
    print(f"æœ€ä½å¥–åŠ±: {np.min(episode_rewards):.2f}")
    print("=" * 60)
    
    env.close()


# ============================================================================
# æ¨¡å¼3: è¿è¡Œæ¨¡å¼
# ============================================================================

def run_model(args):
    """ä½¿ç”¨RLæ¨¡å‹è¿è¡Œä»»åŠ¡"""
    from stable_baselines3 import PPO, SAC
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
    from isaacsim import SimulationApp
    
    print("=" * 60)
    print(f"ğŸ® è¿è¡Œæ¨¡å‹ - {args.model}")
    print("=" * 60)
    
    # å¯åŠ¨Isaac Sim
    simulation_app = SimulationApp({"headless": args.headless})
    
    from isaacsim.core.api import World
    from class_taskEnv import taskEnv_SceneSetup
    from class_controller import RLController
    
    # åˆ›å»ºä¸–ç•Œå’Œä»»åŠ¡
    world = World(stage_units_in_meters=1.0)
    world.scene.add_default_ground_plane()
    
    task = taskEnv_SceneSetup(name="env_armPick", cube_num=args.cube_num)
    world.add_task(task)
    world.reset()
    
    # è·å–æœºå™¨äºº
    task_params = task.get_params()
    robot_name = task_params["robot_name"]["value"]
    robot = world.scene.get_object(robot_name)
    
    # åŠ è½½æ¨¡å‹
    print(f"åŠ è½½æ¨¡å‹: {args.model}")
    if args.algorithm == "ppo":
        rl_model = PPO.load(args.model)
    elif args.algorithm == "sac":
        rl_model = SAC.load(args.model)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {args.algorithm}")
    
    # åŠ è½½VecNormalize
    vec_normalize = None
    if args.vec_normalize:
        vec_normalize = VecNormalize.load(
            args.vec_normalize,
            DummyVecEnv([lambda: None])
        )
        vec_normalize.training = False
        vec_normalize.norm_reward = False
        print(f"åŠ è½½VecNormalize: {args.vec_normalize}")
    
    # åˆ›å»ºRLæ§åˆ¶å™¨
    controller = RLController(
        name="rl_controller",
        articulation=robot,
        vec_normalize=vec_normalize
    )
    controller.set_model(rl_model, vec_normalize)
    
    articulation_controller = robot.get_articulation_controller()
    
    # ä»»åŠ¡çŠ¶æ€
    current_cube_idx = 0
    has_grasped = False
    step_count = 0
    cube_names = task.get_cube_names()
    
    print("\nå¼€å§‹è¿è¡Œ...")
    print("æŒ‰ Ctrl+C åœæ­¢")
    print("-" * 60)
    
    reset_needed = False
    
    try:
        while simulation_app.is_running():
            world.step(render=True)
            
            if world.is_stopped() and not reset_needed:
                reset_needed = True
                
            if world.is_playing():
                # è·å–è§‚æµ‹
                observations = task.get_observations()
                
                # å‡†å¤‡RLæ§åˆ¶å™¨æ‰€éœ€çš„è§‚æµ‹ (ä¿®æ­£ç‰ˆ)
                if current_cube_idx < len(cube_names):
                    current_cube_name = cube_names[current_cube_idx]
                    cube_position = observations[current_cube_name]["position"]
                    # BUGä¿®å¤: é”®åæ˜¯ "color_idx", ä¸æ˜¯ "color"
                    cube_color_idx = observations[current_cube_name]["color_idx"]
                    target_position = observations["target_positions"][cube_color_idx]
                    
                    # å¡«å……æ§åˆ¶å™¨éœ€è¦çš„æ‰€æœ‰ä¿¡æ¯
                    observations["current_cube_position"] = cube_position
                    observations["current_target_position"] = target_position
                    # gripper_state éœ€è¦åœ¨å¾ªç¯ä¸­è‡ªå·±ç»´æŠ¤ï¼Œå°±åƒè®­ç»ƒæ—¶ä¸€æ ·
                    observations["gripper_state"] = 1.0 if has_grasped else 0.0
                
                # ä½¿ç”¨RLæ§åˆ¶å™¨
                actions = controller.forward(observations=observations)
                articulation_controller.apply_action(actions)
                
                step_count += 1
                
                # æ›´æ–° has_grasped çŠ¶æ€ (ç®€åŒ–ç‰ˆé€»è¾‘)
                ee_pos, _ = robot.end_effector.get_world_pose()
                cube_pos = observations.get("current_cube_position", np.zeros(3))
                dist_ee_cube = np.linalg.norm(ee_pos - cube_pos)
                
                if not has_grasped and dist_ee_cube < 0.05 and robot.gripper.is_closed() and cube_pos[2] > 0.02:
                    has_grasped = True
                    print(">> [RUN MODE] Grasped Cube")
                
                if has_grasped and cube_pos[2] < 0.02:
                    has_grasped = False
                    print(">> [RUN MODE] Released Cube")
                # ... (rest of the loop) ...
    
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
    finally:
        simulation_app.close()
        print("ç¨‹åºç»“æŸ")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="RLä¸€ä½“åŒ–è„šæœ¬ - è®­ç»ƒã€æµ‹è¯•ã€è¿è¡Œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  è®­ç»ƒæ¨¡å‹:
    python rl_all_in_one.py train --algorithm ppo --timesteps 100000
    
  æµ‹è¯•æ¨¡å‹:
    python rl_all_in_one.py test --model ./models/ppo_armpickplace_final.zip --episodes 5
    
  è¿è¡Œæ¨¡å‹:
    python rl_all_in_one.py run --model ./models/ppo_armpickplace_final.zip
        """
    )
    
    subparsers = parser.add_subparsers(dest='mode', help='é€‰æ‹©æ¨¡å¼')
    
    # è®­ç»ƒæ¨¡å¼å‚æ•°
    train_parser = subparsers.add_parser('train', help='è®­ç»ƒæ¨¡å¼')
    train_parser.add_argument('--algorithm', type=str, default='ppo', 
                             choices=['ppo', 'sac'], help='RLç®—æ³•')
    train_parser.add_argument('--timesteps', type=int, default=100000, 
                             help='è®­ç»ƒæ­¥æ•°')
    train_parser.add_argument('--cube-num', type=int, default=1, 
                             help='æ–¹å—æ•°é‡')
    train_parser.add_argument('--headless', action='store_true', 
                             help='æ— å¤´æ¨¡å¼')
    train_parser.add_argument('--resume-from', type=str, default=None,
                             help='å·²æœ‰æ¨¡å‹checkpointè·¯å¾„,ç”¨äºç»§ç»­è®­ç»ƒ')
    train_parser.add_argument('--resume-vecnormalize', type=str, default=None,
                             help='VecNormalizeçŠ¶æ€æ–‡ä»¶è·¯å¾„,æœªæä¾›åˆ™å°è¯•è‡ªåŠ¨æ¨æ–­')
    
    # æµ‹è¯•æ¨¡å¼å‚æ•°
    test_parser = subparsers.add_parser('test', help='æµ‹è¯•æ¨¡å¼')
    test_parser.add_argument('--model', type=str, required=True, 
                            help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    test_parser.add_argument('--vec-normalize', type=str, default=None, 
                            help='VecNormalizeæ–‡ä»¶è·¯å¾„')
    test_parser.add_argument('--algorithm', type=str, default='ppo', 
                            choices=['ppo', 'sac'], help='ç®—æ³•ç±»å‹')
    test_parser.add_argument('--episodes', type=int, default=5, 
                            help='æµ‹è¯•å›åˆæ•°')
    test_parser.add_argument('--cube-num', type=int, default=6, 
                            help='æ–¹å—æ•°é‡')
    test_parser.add_argument('--headless', action='store_true', 
                            help='æ— å¤´æ¨¡å¼')
    
    # è¿è¡Œæ¨¡å¼å‚æ•°
    run_parser = subparsers.add_parser('run', help='è¿è¡Œæ¨¡å¼')
    run_parser.add_argument('--model', type=str, required=True, 
                           help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    run_parser.add_argument('--vec-normalize', type=str, default=None, 
                           help='VecNormalizeæ–‡ä»¶è·¯å¾„')
    run_parser.add_argument('--algorithm', type=str, default='ppo', 
                           choices=['ppo', 'sac'], help='ç®—æ³•ç±»å‹')
    run_parser.add_argument('--cube-num', type=int, default=6, 
                           help='æ–¹å—æ•°é‡')
    run_parser.add_argument('--headless', action='store_true', 
                           help='æ— å¤´æ¨¡å¼')
    
    args = parser.parse_args()
    
    if not args.mode:
        parser.print_help()
        return
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
    if args.mode == 'train':
        train_model(args)
    elif args.mode == 'test':
        test_model(args)
    elif args.mode == 'run':
        run_model(args)


if __name__ == "__main__":
    main()
